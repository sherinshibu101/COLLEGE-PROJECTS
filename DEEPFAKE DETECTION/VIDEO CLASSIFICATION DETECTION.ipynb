{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN5JoJ7DPQ8q",
        "outputId": "466fc2bf-b278-450e-ff0a-67845cb074a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AUDIO"
      ],
      "metadata": {
        "id": "cNMTwJyRBLyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements\n",
        "!pip install torch torchaudio tqdm scikit-learn\n",
        "!apt install ffmpeg\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import numpy as np\n",
        "import subprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_LENGTH = 5 * SAMPLE_RATE\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 50\n",
        "ADV_WEIGHT = 0.2\n",
        "\n",
        "class RobustRawNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Main pathway\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv1d(1, 128, kernel_size=51, padding=25),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )\n",
        "\n",
        "        self.res_blocks = nn.Sequential(\n",
        "            self._make_res_block(128, 128, 27),\n",
        "            self._make_res_block(128, 256, 15),\n",
        "            self._make_res_block(256, 256, 15)\n",
        "        )\n",
        "\n",
        "        # Enhanced temporal modeling\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=256,\n",
        "            hidden_size=1024,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048, 256),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    def _make_res_block(self, in_ch, out_ch, kernel_size):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv1d(in_ch, out_ch, kernel_size, padding=kernel_size//2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv1d(out_ch, out_ch, kernel_size, padding=kernel_size//2),\n",
        "            nn.BatchNorm1d(out_ch),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.MaxPool1d(3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.initial(x)\n",
        "        x = self.res_blocks(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x, _ = self.gru(x)\n",
        "        x = x[:, -1, :]\n",
        "        return self.classifier(x)\n",
        "\n",
        "class MP4AudioDataset(Dataset):\n",
        "    def __init__(self, base_path, real_dir, fake_dir):\n",
        "        self.real_path = os.path.join(base_path, real_dir)\n",
        "        self.fake_path = os.path.join(base_path, fake_dir)\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load MP4 files\n",
        "        for dir_path, label in [(self.real_path, 1), (self.fake_path, 0)]:\n",
        "            if os.path.exists(dir_path):\n",
        "                for f in os.listdir(dir_path):\n",
        "                    if f.lower().endswith('.mp4'):\n",
        "                        self.file_paths.append(os.path.join(dir_path, f))\n",
        "                        self.labels.append(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        while True:\n",
        "            try:\n",
        "                path = self.file_paths[idx]\n",
        "                label = self.labels[idx]\n",
        "\n",
        "                # FFmpeg extraction with validation\n",
        "                temp_wav = f\"temp_{os.getpid()}.wav\"\n",
        "                cmd = [\n",
        "                    'ffmpeg', '-y', '-i', path,\n",
        "                    '-ac', '1', '-ar', str(SAMPLE_RATE),\n",
        "                    '-t', '5', '-loglevel', 'error',\n",
        "                    temp_wav\n",
        "                ]\n",
        "                result = subprocess.run(cmd, stderr=subprocess.PIPE)\n",
        "\n",
        "                if result.returncode != 0:\n",
        "                    raise RuntimeError(f\"FFmpeg error: {result.stderr.decode()}\")\n",
        "\n",
        "                # Load and validate audio\n",
        "                waveform, sr = torchaudio.load(temp_wav)\n",
        "                os.remove(temp_wav)\n",
        "\n",
        "                if waveform.nelement() == 0:\n",
        "                    raise ValueError(\"Empty audio file\")\n",
        "\n",
        "                # Process waveform\n",
        "                waveform = waveform.mean(dim=0, keepdim=True)  # Force mono\n",
        "                waveform = waveform / (waveform.abs().max() + 1e-9)\n",
        "\n",
        "                # Pad/trim\n",
        "                if waveform.shape[1] > MAX_LENGTH:\n",
        "                    waveform = waveform[:, :MAX_LENGTH]\n",
        "                else:\n",
        "                    pad = MAX_LENGTH - waveform.shape[1]\n",
        "                    waveform = F.pad(waveform, (0, pad))\n",
        "\n",
        "                return waveform, torch.tensor(label)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Skipping {path}: {str(e)}\")\n",
        "                idx = (idx + 1) % len(self)\n",
        "\n",
        "# Initialize dataset\n",
        "dataset = MP4AudioDataset(\n",
        "    base_path=\"/content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD\",\n",
        "    real_dir=\"videos_real\",\n",
        "    fake_dir=\"videos_fake\"\n",
        ")\n",
        "\n",
        "# Split dataset\n",
        "train_idx, test_idx = train_test_split(\n",
        "    range(len(dataset)),\n",
        "    test_size=0.2,\n",
        "    stratify=dataset.labels,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create loaders\n",
        "train_loader = DataLoader(\n",
        "    torch.utils.data.Subset(dataset, train_idx),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    torch.utils.data.Subset(dataset, test_idx),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True\n",
        ")\n",
        "\n",
        "# Training setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = RobustRawNet().to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "# Training loop\n",
        "best_acc = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "    for waveforms, labels in pbar:\n",
        "        waveforms = waveforms.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(waveforms)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Acc': f\"{100*correct/total:.2f}%\"})\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for waveforms, labels in test_loader:\n",
        "            waveforms = waveforms.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(waveforms)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_acc = test_correct / test_total\n",
        "    print(f\"Test Acc: {100*test_acc:.2f}%\")\n",
        "    scheduler.step()\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save(model.state_dict(), \"best_deepfake_detector.pth\")\n",
        "        print(\"Saved new best model\")\n",
        "\n",
        "print(f\"\\nBest Test Accuracy: {100*best_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "40YqC8lZQQET",
        "outputId": "327d8036-fa4b-4dcd-f7ee-8f0548853d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v27.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v28.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v29.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v15.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v49.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v31.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v32.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v11.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v33.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v30.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v34.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v42.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v35.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v50.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v36.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v52.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v37.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v1.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v38.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v12.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v39.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v14.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v4.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v16.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v17.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v40.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v18.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v41.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v19.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v43.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v2.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v44.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v20.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v45.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v46.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v22.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v47.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v23.mp4: FFmpeg error: Output file #0 does not contain any stream\n",
            "\n",
            "Skipping /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real/v24.mp4: FFmpeg error: Output file #0 does not contain any stream\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/3 [00:12<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-1843086976>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mwaveforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0mwaveforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaveforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj7EGbWWx6no"
      },
      "source": [
        "VIDEOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQE9vS7sz1IT",
        "outputId": "bb58755b-7513-41c6-a3fd-9fe87272a60b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "648f506c437541fc9f42c53b9c186854",
            "3f8c8d045a99412a9de2e9dc908c8bb2",
            "8f317aa33a014ce99d00f940f3e9a1c8",
            "8643e24a5e2c49438c0025926eae8fb4",
            "e5efee7618484d78b632b2110a4237fa",
            "5815d02e630f44d4b16e8833c3314e6e",
            "2bbd09b25aa14b5c822c86ed6141e002",
            "c4e10e9ee2dd46fc898b4834403db5cb",
            "86b54ca5ee4447f6a6d31eeb49379f0b",
            "8afbca4cd01c413a860bb0a74134da0e",
            "ca03098458fb45ffa5049ebda30c08a5",
            "79748f48d5f74af698ba5490ea0ebe34",
            "fa9ffe189d364e38b3fc5bc3b1ea6603",
            "d2af59ae1fae49eb877a7a03e6bb93a9",
            "6074304b84dc4d83a3f811a4b4ac1c04",
            "fbd8803c4d174dc2b8a3a8e485b16233",
            "e92c8506503842ec8fdf4ccee47a9029",
            "060bc8c3b4e24e09a5fa305c1f4e1a3a",
            "d1a43f009e41463db9d4c5d0723139e9",
            "c6154bc4cc6949caa929c27273eb2ea6",
            "8c685cc5c19f4ef08decd16b9e5e4c8e",
            "dfb425b6a2484672b0f999962701e205"
          ]
        },
        "id": "PfC53kz93Msb",
        "outputId": "1ff2d836-22cd-4f6a-9645-d3986ea28c76"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_real...\n",
            "Processing /content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD/videos_fake...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-17 03:26:38,076] A new study created in memory with name: no-name-b00555fa-4a4f-470f-af0a-546496c20b83\n",
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [15, 30, 45] which is of type list.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [20, 40, 60] which is of type list.\n",
            "  warnings.warn(message)\n",
            "/usr/local/lib/python3.11/dist-packages/optuna/distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [25, 50, 75] which is of type list.\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "648f506c437541fc9f42c53b9c186854",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/69.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type deit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79748f48d5f74af698ba5490ea0ebe34",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/23.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-tiny-distilled-patch16-224 and are newly initialized: ['encoder.layer.9.attention.attention.value.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'layernorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.bias', 'layernorm.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.attention.attention.value.weight', 'pooler.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'embeddings.position_embeddings', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.attention.value.bias', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Loss: 0.6575 | Val Acc: 0.5015\n",
            "Epoch 2/50 | Loss: 0.6508 | Val Acc: 0.5015\n",
            "Epoch 3/50 | Loss: 0.6483 | Val Acc: 0.5015\n",
            "Epoch 4/50 | Loss: 0.6503 | Val Acc: 0.5015\n",
            "Epoch 5/50 | Loss: 0.6493 | Val Acc: 0.5015\n",
            "Epoch 6/50 | Loss: 0.6479 | Val Acc: 0.5015\n",
            "Epoch 7/50 | Loss: 0.6511 | Val Acc: 0.5015\n",
            "Epoch 8/50 | Loss: 0.6478 | Val Acc: 0.5015\n",
            "Epoch 9/50 | Loss: 0.6502 | Val Acc: 0.5015\n",
            "Epoch 10/50 | Loss: 0.6471 | Val Acc: 0.5015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-17 04:14:56,240] Trial 0 finished with value: 0.5014577259475219 and parameters: {'learning_rate': 5.6115164153345e-05, 'weight_decay': 0.006351221010640704, 'dropout_rate': 0.39279757672456206, 'unfreeze_schedule': [15, 30, 45]}. Best is trial 0 with value: 0.5014577259475219.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 | Loss: 0.6482 | Val Acc: 0.5015\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type deit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-tiny-distilled-patch16-224 and are newly initialized: ['encoder.layer.9.attention.attention.value.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'layernorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.bias', 'layernorm.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.attention.attention.value.weight', 'pooler.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'embeddings.position_embeddings', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.attention.value.bias', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Loss: 0.6890 | Val Acc: 0.5015\n",
            "Epoch 2/50 | Loss: 0.6671 | Val Acc: 0.5015\n",
            "Epoch 3/50 | Loss: 0.6586 | Val Acc: 0.5015\n",
            "Epoch 4/50 | Loss: 0.6538 | Val Acc: 0.5015\n",
            "Epoch 5/50 | Loss: 0.6507 | Val Acc: 0.5015\n",
            "Epoch 6/50 | Loss: 0.6488 | Val Acc: 0.5015\n",
            "Epoch 7/50 | Loss: 0.6490 | Val Acc: 0.5015\n",
            "Epoch 8/50 | Loss: 0.6492 | Val Acc: 0.5015\n",
            "Epoch 9/50 | Loss: 0.6496 | Val Acc: 0.5015\n",
            "Epoch 10/50 | Loss: 0.6519 | Val Acc: 0.5015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-17 05:01:21,899] Trial 1 finished with value: 0.5014577259475219 and parameters: {'learning_rate': 1.3066739238053272e-05, 'weight_decay': 0.0029154431891537554, 'dropout_rate': 0.34044600469728353, 'unfreeze_schedule': [25, 50, 75]}. Best is trial 0 with value: 0.5014577259475219.\n",
            "You are using a model of type deit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 | Loss: 0.6496 | Val Acc: 0.5015\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-tiny-distilled-patch16-224 and are newly initialized: ['encoder.layer.9.attention.attention.value.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'layernorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.bias', 'layernorm.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.attention.attention.value.weight', 'pooler.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'embeddings.position_embeddings', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.attention.value.bias', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Loss: 0.6532 | Val Acc: 0.5015\n",
            "Epoch 2/50 | Loss: 0.6496 | Val Acc: 0.5015\n",
            "Epoch 3/50 | Loss: 0.6489 | Val Acc: 0.5015\n",
            "Epoch 4/50 | Loss: 0.6475 | Val Acc: 0.5015\n",
            "Epoch 5/50 | Loss: 0.6476 | Val Acc: 0.5015\n",
            "Epoch 6/50 | Loss: 0.6465 | Val Acc: 0.5015\n",
            "Epoch 7/50 | Loss: 0.6445 | Val Acc: 0.5015\n",
            "Epoch 8/50 | Loss: 0.6437 | Val Acc: 0.5015\n",
            "Epoch 9/50 | Loss: 0.6429 | Val Acc: 0.5015\n",
            "Epoch 10/50 | Loss: 0.6434 | Val Acc: 0.5015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-17 05:47:34,308] Trial 2 finished with value: 0.5014577259475219 and parameters: {'learning_rate': 0.000462258900102083, 'weight_decay': 7.068974950624607e-06, 'dropout_rate': 0.17272998688284025, 'unfreeze_schedule': [25, 50, 75]}. Best is trial 0 with value: 0.5014577259475219.\n",
            "You are using a model of type deit to instantiate a model of type vit. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 | Loss: 0.6418 | Val Acc: 0.5015\n",
            "Early stopping triggered\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at facebook/deit-tiny-distilled-patch16-224 and are newly initialized: ['encoder.layer.9.attention.attention.value.bias', 'encoder.layer.4.layernorm_before.bias', 'encoder.layer.7.attention.output.dense.weight', 'layernorm.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.0.attention.attention.query.weight', 'encoder.layer.8.attention.attention.query.weight', 'encoder.layer.2.layernorm_after.weight', 'encoder.layer.2.attention.attention.value.weight', 'encoder.layer.0.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.6.attention.attention.query.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.layernorm_before.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.8.attention.attention.value.weight', 'encoder.layer.3.attention.attention.query.bias', 'encoder.layer.0.attention.attention.query.bias', 'encoder.layer.10.layernorm_before.bias', 'encoder.layer.9.layernorm_after.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.attention.value.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.10.attention.attention.value.bias', 'layernorm.bias', 'encoder.layer.1.layernorm_after.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'pooler.dense.weight', 'encoder.layer.5.attention.attention.query.bias', 'encoder.layer.11.layernorm_after.weight', 'encoder.layer.11.layernorm_before.weight', 'encoder.layer.11.attention.attention.value.weight', 'pooler.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.attention.key.bias', 'encoder.layer.11.attention.attention.key.weight', 'encoder.layer.7.layernorm_before.weight', 'encoder.layer.5.layernorm_after.bias', 'encoder.layer.7.attention.attention.value.weight', 'encoder.layer.10.attention.attention.query.bias', 'encoder.layer.2.attention.attention.key.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.11.attention.attention.key.bias', 'encoder.layer.2.layernorm_after.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.attention.query.weight', 'encoder.layer.6.layernorm_before.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.attention.value.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.attention.value.bias', 'encoder.layer.6.attention.attention.value.weight', 'encoder.layer.2.attention.attention.query.bias', 'encoder.layer.7.attention.attention.value.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.attention.query.weight', 'encoder.layer.3.layernorm_after.bias', 'encoder.layer.0.layernorm_after.bias', 'encoder.layer.10.attention.attention.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.10.attention.attention.query.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.8.layernorm_before.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.attention.attention.value.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.layernorm_after.weight', 'encoder.layer.1.layernorm_before.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.9.attention.attention.key.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.attention.attention.key.bias', 'encoder.layer.11.output.dense.bias', 'embeddings.patch_embeddings.projection.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.layernorm_after.weight', 'embeddings.cls_token', 'encoder.layer.5.layernorm_after.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.layernorm_before.bias', 'encoder.layer.0.layernorm_before.bias', 'encoder.layer.1.attention.attention.query.bias', 'encoder.layer.1.attention.attention.value.weight', 'encoder.layer.9.attention.attention.query.bias', 'encoder.layer.4.layernorm_before.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.attention.attention.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.0.attention.attention.key.weight', 'encoder.layer.0.attention.attention.value.weight', 'encoder.layer.1.attention.attention.query.weight', 'encoder.layer.10.layernorm_after.weight', 'encoder.layer.0.layernorm_after.weight', 'encoder.layer.3.attention.attention.value.bias', 'encoder.layer.3.layernorm_before.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.weight', 'encoder.layer.4.attention.attention.query.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.attention.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.attention.key.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.attention.key.weight', 'encoder.layer.1.layernorm_after.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'embeddings.position_embeddings', 'encoder.layer.11.layernorm_before.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.attention.key.bias', 'encoder.layer.9.attention.attention.key.bias', 'encoder.layer.10.attention.attention.key.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.5.layernorm_before.weight', 'encoder.layer.10.attention.attention.key.weight', 'encoder.layer.4.layernorm_after.weight', 'encoder.layer.9.layernorm_before.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.attention.key.weight', 'encoder.layer.8.layernorm_after.bias', 'encoder.layer.1.layernorm_before.weight', 'encoder.layer.2.attention.attention.value.bias', 'encoder.layer.7.layernorm_before.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.2.attention.attention.query.weight', 'encoder.layer.4.attention.attention.key.weight', 'encoder.layer.11.attention.attention.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.5.layernorm_before.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.layernorm_after.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.layernorm_after.weight', 'encoder.layer.7.attention.attention.key.bias', 'encoder.layer.5.attention.attention.value.weight', 'encoder.layer.11.layernorm_after.bias', 'encoder.layer.1.attention.attention.key.bias', 'encoder.layer.8.attention.attention.key.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.attention.attention.key.weight', 'encoder.layer.9.attention.attention.query.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.attention.value.bias', 'embeddings.patch_embeddings.projection.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.0.layernorm_before.weight', 'encoder.layer.8.attention.attention.query.bias', 'encoder.layer.7.attention.attention.query.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.layernorm_before.bias', 'encoder.layer.1.attention.attention.value.bias', 'encoder.layer.2.layernorm_before.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.3.attention.attention.key.bias', 'encoder.layer.6.attention.attention.key.bias', 'encoder.layer.8.layernorm_before.bias', 'encoder.layer.4.attention.attention.key.bias', 'encoder.layer.6.attention.attention.query.weight', 'encoder.layer.7.layernorm_after.weight', 'encoder.layer.3.attention.attention.query.weight', 'encoder.layer.7.attention.attention.query.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.7.layernorm_after.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.11.attention.attention.query.bias', 'encoder.layer.4.layernorm_after.bias', 'encoder.layer.9.layernorm_after.bias', 'encoder.layer.10.layernorm_after.bias', 'encoder.layer.2.layernorm_before.bias', 'encoder.layer.5.attention.attention.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.attention.key.weight', 'encoder.layer.3.layernorm_before.weight', 'encoder.layer.3.output.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Loss: 0.6665 | Val Acc: 0.5015\n",
            "Epoch 2/50 | Loss: 0.6530 | Val Acc: 0.5015\n",
            "Epoch 3/50 | Loss: 0.6478 | Val Acc: 0.5015\n",
            "Epoch 4/50 | Loss: 0.6498 | Val Acc: 0.5015\n",
            "Epoch 5/50 | Loss: 0.6468 | Val Acc: 0.5015\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[W 2025-06-17 06:09:37,400] Trial 3 failed with parameters: {'learning_rate': 7.309539835912905e-05, 'weight_decay': 1.461896279370496e-05, 'dropout_rate': 0.34474115788895177, 'unfreeze_schedule': [25, 50, 75]} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 278, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, train_loader, val_loader, device), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 273, in objective\n",
            "    best_val_acc = train_with_progressive_unfreezing(model, train_loader, val_loader, config, device)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 205, in train_with_progressive_unfreezing\n",
            "    logits2 = model(img2)\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 175, in forward\n",
            "    outputs = self.vit(x)\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 577, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 407, in forward\n",
            "    layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 352, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "                             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 294, in forward\n",
            "    self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 212, in forward\n",
            "    mixed_query_layer = self.query(hidden_states)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-06-17 06:09:37,427] Trial 3 failed with value None.\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-1-3088107041>\", line 334, in <cell line: 0>\n",
            "    best_params = run_hyperparameter_search(train_loader, val_loader, device, n_trials=10)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 278, in run_hyperparameter_search\n",
            "    study.optimize(lambda trial: objective(trial, train_loader, val_loader, device), n_trials=n_trials)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\", line 489, in optimize\n",
            "    _optimize(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 64, in _optimize\n",
            "    _optimize_sequential(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 161, in _optimize_sequential\n",
            "    frozen_trial = _run_trial(study, func, catch)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 253, in _run_trial\n",
            "    raise func_err\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 201, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 278, in <lambda>\n",
            "    study.optimize(lambda trial: objective(trial, train_loader, val_loader, device), n_trials=n_trials)\n",
            "                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 273, in objective\n",
            "    best_val_acc = train_with_progressive_unfreezing(model, train_loader, val_loader, config, device)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 205, in train_with_progressive_unfreezing\n",
            "    logits2 = model(img2)\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<ipython-input-1-3088107041>\", line 175, in forward\n",
            "    outputs = self.vit(x)\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 577, in forward\n",
            "    encoder_outputs = self.encoder(\n",
            "                      ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 407, in forward\n",
            "    layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 352, in forward\n",
            "    self_attention_outputs = self.attention(\n",
            "                             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 294, in forward\n",
            "    self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\", line 212, in forward\n",
            "    mixed_query_layer = self.query(hidden_states)\n",
            "                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\", line 116, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 987, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "                           ^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3088107041>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;31m# Run hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_hyperparameter_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3088107041>\u001b[0m in \u001b[0;36mrun_hyperparameter_search\u001b[0;34m(train_loader, val_loader, device, n_trials)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_startup_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best hyperparameters: {study.best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \"\"\"\n\u001b[0;32m--> 489\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     65\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    252\u001b[0m     ):\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3088107041>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMedianPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_startup_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_warmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best hyperparameters: {study.best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3088107041>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial, train_loader, val_loader, device)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_progressive_unfreezing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbest_val_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3088107041>\u001b[0m in \u001b[0;36mtrain_with_progressive_unfreezing\u001b[0;34m(model, train_loader, val_loader, config, device)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mlogits1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mlogits2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-3088107041>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, return_attn)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mpooled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# CLS token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    351\u001b[0m     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[0;32m--> 352\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_before\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# in ViT, layernorm is applied before self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    293\u001b[0m     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mself_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as T\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "import json\n",
        "\n",
        "# ================== Frame Extraction Functions ==================\n",
        "def extract_frames(video_path, output_dir, frames_per_video=3):\n",
        "    \"\"\"Extract frames from video file\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    interval = max(1, total_frames // frames_per_video)\n",
        "\n",
        "    for i in range(frames_per_video):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, i * interval)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            frame_path = os.path.join(\n",
        "                output_dir,\n",
        "                f\"{os.path.basename(video_path).split('.')[0]}_frame_{i:03d}.jpg\"\n",
        "            )\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "    cap.release()\n",
        "\n",
        "def process_dataset(input_dir, output_dir, frames_per_video=3):\n",
        "    \"\"\"Process directory of videos into frames\"\"\"\n",
        "    print(f\"Processing {input_dir}...\")\n",
        "    for video_file in os.listdir(input_dir):\n",
        "        video_path = os.path.join(input_dir, video_file)\n",
        "        if os.path.isfile(video_path):\n",
        "            extract_frames(video_path, output_dir, frames_per_video)\n",
        "\n",
        "# ================== Dataset and Model Classes ==================\n",
        "class DeepfakeContrastiveDataset(Dataset):\n",
        "    def __init__(self, real_dir, fake_dir, transform=None, mode='train'):\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.real_images = []\n",
        "        self.fake_images = []\n",
        "\n",
        "        # Get unique video IDs\n",
        "        real_videos = sorted({f.split('_')[0] for f in os.listdir(real_dir)})\n",
        "        fake_videos = sorted({f.split('_')[0] for f in os.listdir(fake_dir)})\n",
        "\n",
        "        # Split videos (70% train, 15% val, 15% test)\n",
        "        real_train, real_temp = train_test_split(real_videos, test_size=0.3, random_state=42)\n",
        "        real_val, real_test = train_test_split(real_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "        fake_train, fake_temp = train_test_split(fake_videos, test_size=0.3, random_state=42)\n",
        "        fake_val, fake_test = train_test_split(fake_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "        # Select split\n",
        "        if mode == 'train':\n",
        "            real_vids, fake_vids = real_train, fake_train\n",
        "        elif mode == 'val':\n",
        "            real_vids, fake_vids = real_val, fake_val\n",
        "        elif mode == 'test':\n",
        "            real_vids, fake_vids = real_test, fake_test\n",
        "\n",
        "        # Load frames\n",
        "        self._load_frames(real_dir, real_vids, self.real_images)\n",
        "        self._load_frames(fake_dir, fake_vids, self.fake_images)\n",
        "\n",
        "        # Create contrastive pairs for training\n",
        "        if mode == 'train':\n",
        "            self.pairs = []\n",
        "            # Real-Fake pairs\n",
        "            for _ in range(int(len(self.real_images)*1.4)):\n",
        "                self.pairs.append((\n",
        "                    random.choice(self.real_images),\n",
        "                    random.choice(self.fake_images),\n",
        "                    0\n",
        "                ))\n",
        "            # Real-Real pairs\n",
        "            for _ in range(int(len(self.real_images)*0.6)):\n",
        "                self.pairs.append((\n",
        "                    random.choice(self.real_images),\n",
        "                    random.choice(self.real_images),\n",
        "                    1\n",
        "                ))\n",
        "            random.shuffle(self.pairs)\n",
        "\n",
        "    def _load_frames(self, root_dir, video_list, target_list):\n",
        "        \"\"\"Load frames for specified videos\"\"\"\n",
        "        for f in os.listdir(root_dir):\n",
        "            video_id = f.split('_')[0]\n",
        "            if video_id in video_list:\n",
        "                target_list.append(os.path.join(root_dir, f))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs) if self.mode == 'train' else len(self.real_images) + len(self.fake_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.mode != 'train':\n",
        "            # Validation/Test mode\n",
        "            if idx < len(self.real_images):\n",
        "                img_path = self.real_images[idx]\n",
        "                label = 1\n",
        "            else:\n",
        "                img_path = self.fake_images[idx - len(self.real_images)]\n",
        "                label = 0\n",
        "\n",
        "            img = cv2.imread(img_path)\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            if self.transform:\n",
        "                img = self.transform(image=img)['image']\n",
        "            return img, torch.tensor(label, dtype=torch.long)\n",
        "        else:\n",
        "            # Training mode with contrastive pairs\n",
        "            img1_path, img2_path, label = self.pairs[idx]\n",
        "            img1 = cv2.imread(img1_path)\n",
        "            img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "            img2 = cv2.imread(img2_path)\n",
        "            img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            if self.transform:\n",
        "                img1 = self.transform(image=img1)['image']\n",
        "                img2 = self.transform(image=img2)['image']\n",
        "\n",
        "            return img1, img2, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "class EnhancedDeepfakeClassifier(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        vit_config = ViTConfig.from_pretrained(\"facebook/deit-tiny-distilled-patch16-224\")\n",
        "        vit_config.output_attentions = True\n",
        "        self.vit = ViTModel.from_pretrained(\"facebook/deit-tiny-distilled-patch16-224\", config=vit_config)\n",
        "\n",
        "        # Freeze initial layers\n",
        "        for param in self.vit.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Progressive unfreezing setup\n",
        "        self.layer_groups = {\n",
        "            'final_layers': ['encoder.layer.9', 'encoder.layer.10', 'encoder.layer.11'],\n",
        "            'mid_layers': ['encoder.layer.6', 'encoder.layer.7', 'encoder.layer.8'],\n",
        "            'early_layers': ['encoder.layer.3', 'encoder.layer.4', 'encoder.layer.5']\n",
        "        }\n",
        "        self.config = config\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(192, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(config.get('DROPOUT_RATE', 0.2)),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.get('DROPOUT_RATE', 0.2)),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def unfreeze_layer_group(self, group_name):\n",
        "        \"\"\"Unfreeze specified layer group\"\"\"\n",
        "        for name, param in self.vit.named_parameters():\n",
        "            if any(layer in name for layer in self.layer_groups[group_name]):\n",
        "                param.requires_grad = True\n",
        "        print(f\"Unfroze layer group: {group_name}\")\n",
        "\n",
        "    def forward(self, x, return_attn=False):\n",
        "        outputs = self.vit(x)\n",
        "        pooled = outputs.last_hidden_state[:, 0]  # CLS token\n",
        "        logits = self.classifier(pooled)\n",
        "        return (logits, outputs.attentions) if return_attn else logits\n",
        "\n",
        "# ================== Training and Evaluation ==================\n",
        "def train_with_progressive_unfreezing(model, train_loader, val_loader, config, device):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config['LEARNING_RATE'], weight_decay=config['WEIGHT_DECAY'])\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=20)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(config['NUM_EPOCHS']):\n",
        "        # Progressive unfreezing\n",
        "        if epoch in config['UNFREEZE_SCHEDULE']:\n",
        "            groups = ['final_layers', 'mid_layers', 'early_layers']\n",
        "            idx = config['UNFREEZE_SCHEDULE'].index(epoch)\n",
        "            if idx < len(groups):\n",
        "                model.unfreeze_layer_group(groups[idx])\n",
        "\n",
        "        # Training loop\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch_idx, (img1, img2, labels) in enumerate(train_loader):\n",
        "            img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
        "\n",
        "            # Forward passes\n",
        "            logits1 = model(img1)\n",
        "            logits2 = model(img2)\n",
        "\n",
        "            # Loss calculation\n",
        "            loss1 = criterion(logits1, torch.ones_like(labels).long())\n",
        "            loss2 = criterion(logits2, (labels == 1).long())\n",
        "            loss = (loss1 + loss2) / 2\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        val_acc = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch+1}/{config['NUM_EPOCHS']} | Loss: {total_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= config['PATIENCE']:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return best_val_acc\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    return correct / total if total > 0 else 0\n",
        "\n",
        "# ================== Hyperparameter Search ==================\n",
        "def get_search_space(trial):\n",
        "    return {\n",
        "        'LEARNING_RATE': trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True),\n",
        "        'WEIGHT_DECAY': trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True),\n",
        "        'DROPOUT_RATE': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
        "        'UNFREEZE_SCHEDULE': trial.suggest_categorical('unfreeze_schedule',\n",
        "            [[15, 30, 45], [20, 40, 60], [25, 50, 75]])\n",
        "    }\n",
        "\n",
        "def objective(trial, train_loader, val_loader, device):\n",
        "    config = {\n",
        "        'NUM_EPOCHS': 50,\n",
        "        'PATIENCE': 10,\n",
        "        'BATCH_SIZE': 16,\n",
        "        'FRAMES_PER_VIDEO': 3,\n",
        "        'IMAGE_SIZE': 224\n",
        "    }\n",
        "    config.update(get_search_space(trial))\n",
        "\n",
        "    model = EnhancedDeepfakeClassifier(config).to(device)\n",
        "\n",
        "    best_val_acc = train_with_progressive_unfreezing(model, train_loader, val_loader, config, device)\n",
        "    return best_val_acc\n",
        "\n",
        "def run_hyperparameter_search(train_loader, val_loader, device, n_trials=20):\n",
        "    study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42), pruner=MedianPruner(n_startup_trials=5, n_warmup_steps=10))\n",
        "    study.optimize(lambda trial: objective(trial, train_loader, val_loader, device), n_trials=n_trials)\n",
        "    print(f\"Best hyperparameters: {study.best_params}\")\n",
        "    print(f\"Best validation accuracy: {study.best_value:.4f}\")\n",
        "    return study.best_params\n",
        "\n",
        "# ================== Main Execution ==================\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    base_config = {\n",
        "        'BATCH_SIZE': 16,\n",
        "        'NUM_EPOCHS': 50,\n",
        "        'LEARNING_RATE': 3e-4,\n",
        "        'WEIGHT_DECAY': 0.01,\n",
        "        'PATIENCE': 10,\n",
        "        'IMAGE_SIZE': 224,\n",
        "        'FRAMES_PER_VIDEO': 3,\n",
        "        'UNFREEZE_SCHEDULE': [15, 30, 45],\n",
        "        'DROPOUT_RATE': 0.3\n",
        "    }\n",
        "\n",
        "    # Setup paths\n",
        "    base_path = \"/content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD\"\n",
        "    real_videos_dir = os.path.join(base_path, \"videos_real\")\n",
        "    fake_videos_dir = os.path.join(base_path, \"videos_fake\")\n",
        "    real_frames_dir = os.path.join(base_path, \"real_frames\")\n",
        "    fake_frames_dir = os.path.join(base_path, \"fake_frames\")\n",
        "\n",
        "    # Process datasets\n",
        "    process_dataset(real_videos_dir, real_frames_dir, base_config['FRAMES_PER_VIDEO'])\n",
        "    process_dataset(fake_videos_dir, fake_frames_dir, base_config['FRAMES_PER_VIDEO'])\n",
        "\n",
        "    # Create datasets\n",
        "    train_transform = A.Compose([\n",
        "        A.RandomResizedCrop(224, 224),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    test_transform = A.Compose([\n",
        "        A.Resize(224, 224),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "    train_dataset = DeepfakeContrastiveDataset(real_frames_dir, fake_frames_dir, train_transform, 'train')\n",
        "    val_dataset = DeepfakeContrastiveDataset(real_frames_dir, fake_frames_dir, test_transform, 'val')\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=base_config['BATCH_SIZE'], shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=base_config['BATCH_SIZE'], num_workers=2)\n",
        "\n",
        "    # Initialize model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Run hyperparameter search\n",
        "    best_params = run_hyperparameter_search(train_loader, val_loader, device, n_trials=10)\n",
        "\n",
        "    # Update config with best params\n",
        "    final_config = base_config.copy()\n",
        "    final_config.update(best_params)\n",
        "\n",
        "    # Train final model\n",
        "    final_model = EnhancedDeepfakeClassifier(final_config).to(device)\n",
        "    train_with_progressive_unfreezing(final_model, train_loader, val_loader, final_config, device)\n",
        "\n",
        "    # Load best model and evaluate on test set\n",
        "    final_model.load_state_dict(torch.load('best_model.pth'))\n",
        "    test_dataset = DeepfakeContrastiveDataset(real_frames_dir, fake_frames_dir, test_transform, 'test')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=final_config['BATCH_SIZE'], num_workers=2)\n",
        "\n",
        "    def evaluate_final(model, loader, device):\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "        acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(all_labels, all_preds, target_names=['Fake', 'Real']))\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Fake', 'Real'])\n",
        "        disp.plot(cmap='Blues')\n",
        "        plt.show()\n",
        "\n",
        "    evaluate_final(final_model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Uninstall all conflicting packages\n",
        "!pip uninstall -y torch torchvision torchaudio albumentations opencv-python facenet-pytorch\n",
        "\n",
        "# Step 2: Install PyTorch 2.2.0+ stack (CUDA 12.1)\n",
        "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121 --force-reinstall\n",
        "\n",
        "# Step 3: Install vision/audio dependencies\n",
        "!pip install albumentations==1.3.1 opencv-python==4.8.0.76 facenet-pytorch==2.5.3\n",
        "\n",
        "# Step 4: Install sklearn and matplotlib\n",
        "!pip install scikit-learn==1.3.2 matplotlib==3.7.1\n",
        "\n",
        "# Step 5: Install Hugging Face Transformers and dependencies\n",
        "!pip install transformers==4.37.0 huggingface_hub==0.23.0 accelerate==0.31.0\n",
        "\n",
        "# Step 6: Fix OpenCV system dependencies\n",
        "!apt-get update -qq && apt-get install -y libgl1-mesa-glx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6k15RBBZvnFG",
        "outputId": "a2cf9e24-4b0f-4c2b-8554-b85c98e37530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.1.0+cu121\n",
            "Uninstalling torch-2.1.0+cu121:\n",
            "  Successfully uninstalled torch-2.1.0+cu121\n",
            "Found existing installation: torchvision 0.16.0+cu121\n",
            "Uninstalling torchvision-0.16.0+cu121:\n",
            "  Successfully uninstalled torchvision-0.16.0+cu121\n",
            "Found existing installation: torchaudio 2.1.0+cu121\n",
            "Uninstalling torchaudio-2.1.0+cu121:\n",
            "  Successfully uninstalled torchaudio-2.1.0+cu121\n",
            "Found existing installation: albumentations 1.3.1\n",
            "Uninstalling albumentations-1.3.1:\n",
            "  Successfully uninstalled albumentations-1.3.1\n",
            "Found existing installation: opencv-python 4.8.0.76\n",
            "Uninstalling opencv-python-4.8.0.76:\n",
            "  Successfully uninstalled opencv-python-4.8.0.76\n",
            "Found existing installation: facenet-pytorch 2.5.3\n",
            "Uninstalling facenet-pytorch-2.5.3:\n",
            "  Successfully uninstalled facenet-pytorch-2.5.3\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp311-cp311-linux_x86_64.whl (757.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m642.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.17.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.17.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.2.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.2.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting typing-extensions>=4.8.0 (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting jinja2 (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting fsspec (from torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m818.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.2.0 (from torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting requests (from torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.8/19.8 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Collecting charset-normalizer<3,>=2 (from requests->torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/idna-3.4-py3-none-any.whl (61 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests->torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->torchvision==0.17.0)\n",
            "  Using cached https://download.pytorch.org/whl/certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.2.0)\n",
            "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
            "Using cached https://download.pytorch.org/whl/typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
            "Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
            "Using cached https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "Using cached https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
            "Using cached https://download.pytorch.org/whl/numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
            "Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "Installing collected packages: mpmath, urllib3, typing-extensions, sympy, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.13\n",
            "    Uninstalling urllib3-1.26.13:\n",
            "      Successfully uninstalled urllib3-1.26.13\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.3\n",
            "    Uninstalling sympy-1.13.3:\n",
            "      Successfully uninstalled sympy-1.13.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.0.0\n",
            "    Uninstalling pillow-11.0.0:\n",
            "      Successfully uninstalled pillow-11.0.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.3\n",
            "    Uninstalling networkx-3.3:\n",
            "      Successfully uninstalled networkx-3.3\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.13.1\n",
            "    Uninstalling filelock-3.13.1:\n",
            "      Successfully uninstalled filelock-3.13.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 2.1.1\n",
            "    Uninstalling charset-normalizer-2.1.1:\n",
            "      Successfully uninstalled charset-normalizer-2.1.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.12.7\n",
            "    Uninstalling certifi-2022.12.7:\n",
            "      Successfully uninstalled certifi-2022.12.7\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.28.1\n",
            "    Uninstalling requests-2.28.1:\n",
            "      Successfully uninstalled requests-2.28.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n",
            "scikit-learn 1.3.2 requires numpy<2.0,>=1.17.3, but you have numpy 2.1.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.28.1 which is incompatible.\n",
            "sphinx 8.2.3 requires requests>=2.30.0, but you have requests 2.28.1 which is incompatible.\n",
            "curl-cffi 0.11.3 requires certifi>=2024.2.2, but you have certifi 2022.12.7 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.1.2 which is incompatible.\n",
            "yfinance 0.2.62 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "pytensor 2.31.3 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 certifi-2022.12.7 charset-normalizer-2.1.1 filelock-3.13.1 fsspec-2024.6.1 idna-3.4 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 numpy-2.1.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 pillow-11.0.0 requests-2.28.1 sympy-1.13.3 torch-2.2.0+cu121 torchaudio-2.2.0+cu121 torchvision-0.17.0+cu121 triton-2.2.0 typing-extensions-4.12.2 urllib3-1.26.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "charset_normalizer",
                  "filelock",
                  "idna",
                  "markupsafe",
                  "mpmath",
                  "numpy",
                  "requests",
                  "sympy",
                  "torch",
                  "torchgen",
                  "torchvision",
                  "urllib3"
                ]
              },
              "id": "88392a3243d848448e0a87ee951974be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting albumentations==1.3.1\n",
            "  Using cached albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting opencv-python==4.8.0.76\n",
            "  Using cached opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting facenet-pytorch==2.5.3\n",
            "  Using cached facenet_pytorch-2.5.3-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (2.1.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (1.15.3)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (0.25.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (6.0.2)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.3.1) (4.11.0.86)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch==2.5.3) (2.28.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch==2.5.3) (0.17.0+cu121)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from facenet-pytorch==2.5.3) (11.0.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (1.3.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qudida>=0.0.4->albumentations==1.3.1) (4.12.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (3.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (2025.6.1)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.1) (0.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->facenet-pytorch==2.5.3) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->facenet-pytorch==2.5.3) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->facenet-pytorch==2.5.3) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->facenet-pytorch==2.5.3) (2022.12.7)\n",
            "Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->facenet-pytorch==2.5.3) (2.2.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (12.1.105)\n",
            "Collecting numpy>=1.11.1 (from albumentations==1.3.1)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.1) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0->torchvision->facenet-pytorch==2.5.3) (1.3.0)\n",
            "Using cached albumentations-1.3.1-py3-none-any.whl (125 kB)\n",
            "Using cached opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "Using cached facenet_pytorch-2.5.3-py3-none-any.whl (1.9 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy, opencv-python, facenet-pytorch, albumentations\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.1.2\n",
            "    Uninstalling numpy-2.1.2:\n",
            "      Successfully uninstalled numpy-2.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.62 requires requests>=2.31, but you have requests 2.28.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "pytensor 2.31.3 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-1.3.1 facenet-pytorch-2.5.3 numpy-1.26.4 opencv-python-4.8.0.76\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "albumentations",
                  "cv2",
                  "numpy"
                ]
              },
              "id": "ae96d71396e54bdba47b2ad0782c67d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.3.2 in /usr/local/lib/python3.11/dist-packages (1.3.2)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /usr/local/lib/python3.11/dist-packages (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (4.58.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.7.1) (1.17.0)\n",
            "Collecting transformers==4.37.0\n",
            "  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub==0.23.0\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting accelerate==0.31.0\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (2.28.1)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.37.0)\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.0) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.0) (4.12.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.31.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate==0.31.0) (2.2.0+cu121)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate==0.31.0) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.31.0) (12.1.105)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.0) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.0) (1.26.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.31.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->accelerate==0.31.0) (1.3.0)\n",
            "Downloading transformers-4.37.0-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub, tokenizers, transformers, accelerate\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.33.0\n",
            "    Uninstalling huggingface-hub-0.33.0:\n",
            "      Successfully uninstalled huggingface-hub-0.33.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.7.0\n",
            "    Uninstalling accelerate-1.7.0:\n",
            "      Successfully uninstalled accelerate-1.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.37.0 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "diffusers 0.33.1 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.31.0 huggingface_hub-0.23.0 tokenizers-0.15.2 transformers-4.37.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "edbce374317f4983b1b094e5d0c74fda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 46 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import ViTModel, ViTConfig\n",
        "\n",
        "# Configuration\n",
        "FRAME_SIZE = 224\n",
        "CLIP_LENGTH = 4\n",
        "BATCH_SIZE = 4  # Lowered for RAM efficiency\n",
        "EPOCHS = 5     # Use more if you have time/resources\n",
        "NUM_FOLDS = 5\n",
        "SEED = 42\n",
        "\n",
        "class VideoTransform:\n",
        "    def __init__(self, train=True):\n",
        "        self.spatial_transform = A.Compose([\n",
        "            A.RandomResizedCrop(FRAME_SIZE, FRAME_SIZE, scale=(0.2, 1.0)),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.3, contrast=0.3, p=0.7),\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=0.4),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ]) if train else A.Compose([\n",
        "            A.Resize(FRAME_SIZE, FRAME_SIZE),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    def __call__(self, frames):\n",
        "        transformed = []\n",
        "        for frame in frames:\n",
        "            transformed.append(self.spatial_transform(image=frame)['image'])\n",
        "        return torch.stack(transformed)\n",
        "\n",
        "class VideoDeepfakeDataset(Dataset):\n",
        "    def __init__(self, base_path, real_dir=\"videos_real\", fake_dir=\"videos_fake\", transform=None):\n",
        "        self.real_path = os.path.join(base_path, real_dir)\n",
        "        self.fake_path = os.path.join(base_path, fake_dir)\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "        for f in os.listdir(self.real_path):\n",
        "            if f.endswith(\".mp4\"):\n",
        "                self.file_paths.append(os.path.join(self.real_path, f))\n",
        "                self.labels.append(1)\n",
        "        for f in os.listdir(self.fake_path):\n",
        "            if f.endswith(\".mp4\"):\n",
        "                self.file_paths.append(os.path.join(self.fake_path, f))\n",
        "                self.labels.append(0)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "            if len(frames) >= CLIP_LENGTH * 2:\n",
        "                break\n",
        "        cap.release()\n",
        "        clip = self._sample_clip(frames, CLIP_LENGTH)\n",
        "        if self.transform:\n",
        "            clip = self.transform(clip)\n",
        "        return clip, torch.tensor(label)\n",
        "    def _sample_clip(self, frames, clip_length):\n",
        "        if len(frames) < clip_length:\n",
        "            frames += [frames[-1]] * (clip_length - len(frames))\n",
        "        start_idx = np.random.randint(0, max(1, len(frames) - clip_length))\n",
        "        return frames[start_idx : start_idx + clip_length]\n",
        "\n",
        "class MultiScaleLocalAttention(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_channels=32):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 5, padding=2),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 7, padding=3),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(base_channels * 3, base_channels, 1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    def forward(self, x):\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        x_cat = torch.cat([x1, x2, x3], dim=1)\n",
        "        x_fused = self.fuse(x_cat)\n",
        "        pooled = self.pool(x_fused).flatten(1)\n",
        "        return pooled\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, global_dim, local_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(global_dim, out_dim)\n",
        "        self.key_proj = nn.Linear(local_dim, out_dim)\n",
        "        self.value_proj = nn.Linear(local_dim, out_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        Q = self.query_proj(global_feat).unsqueeze(1)\n",
        "        K = self.key_proj(local_feat).unsqueeze(1)\n",
        "        V = self.value_proj(local_feat).unsqueeze(1)\n",
        "        attn_weights = self.softmax(torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(Q.shape[-1]))\n",
        "        fused = torch.bmm(attn_weights, V).squeeze(1)\n",
        "        return fused\n",
        "\n",
        "class HybridFrameFeature(nn.Module):\n",
        "    def __init__(self, vit_ckpt=\"WinKawaks/vit-tiny-patch16-224\", local_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        config = ViTConfig.from_pretrained(vit_ckpt)\n",
        "        self.vit = ViTModel.from_pretrained(vit_ckpt, config=config)\n",
        "        self.local_attn = MultiScaleLocalAttention(in_channels=3, base_channels=local_dim)\n",
        "        self.cross_attn = CrossAttentionFusion(global_dim=config.hidden_size, local_dim=local_dim, out_dim=fusion_dim)\n",
        "        self.out_dim = config.hidden_size + fusion_dim\n",
        "    def forward(self, x):\n",
        "        global_feat = self.vit(x).last_hidden_state[:, 0]\n",
        "        local_feat = self.local_attn(x)\n",
        "        fused = self.cross_attn(global_feat, local_feat)\n",
        "        return torch.cat([global_feat, fused], dim=1)\n",
        "\n",
        "class VideoDeepfakeClassifier(nn.Module):\n",
        "    def __init__(self, frame_model, frame_feat_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.frame_model = frame_model\n",
        "        self.temporal_attn = nn.MultiheadAttention(\n",
        "            embed_dim=frame_feat_dim,\n",
        "            num_heads=4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(frame_feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape[:2]\n",
        "        x = x.view(B * T, *x.shape[2:])\n",
        "        frame_feats = self.frame_model(x)\n",
        "        frame_feats = frame_feats.view(B, T, -1)\n",
        "        attn_out, _ = self.temporal_attn(frame_feats, frame_feats, frame_feats)\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# Prepare dataset and labels for splitting\n",
        "base_path = \"/content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD\"\n",
        "dataset = VideoDeepfakeDataset(base_path, transform=VideoTransform(train=True))\n",
        "labels = np.array(dataset.labels)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{NUM_FOLDS} ---\")\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    frame_model = HybridFrameFeature()\n",
        "    frame_feat_dim = frame_model.out_dim\n",
        "    model = VideoDeepfakeClassifier(frame_model, frame_feat_dim).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
        "        for clips, labels in pbar:\n",
        "            clips = clips.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Acc': f\"{100*correct/total:.2f}%\"})\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for clips, labels in val_loader:\n",
        "                clips = clips.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(clips)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"Fold {fold+1} Epoch {epoch+1} Val Acc: {100*val_acc:.2f}%\")\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"best_video_model_hybrid_fold{fold+1}.pth\")\n",
        "            print(\"Saved new best model for this fold\")\n",
        "    print(f\"Best Val Accuracy for Fold {fold+1}: {100*best_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zApB0lPT44cG",
        "outputId": "c97848dc-333b-480b-a135-d8f0f258df9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 1 Epoch 1: 100%|██████████| 21/21 [02:28<00:00,  7.08s/it, Loss=0.6164, Acc=40.48%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 1 Val Acc: 54.55%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 2: 100%|██████████| 21/21 [02:34<00:00,  7.34s/it, Loss=0.7177, Acc=47.62%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 2 Val Acc: 50.00%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 3: 100%|██████████| 21/21 [02:37<00:00,  7.50s/it, Loss=0.7064, Acc=41.67%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 3 Val Acc: 50.00%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 4: 100%|██████████| 21/21 [02:24<00:00,  6.88s/it, Loss=0.7331, Acc=50.00%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 4 Val Acc: 50.00%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 5: 100%|██████████| 21/21 [02:24<00:00,  6.87s/it, Loss=0.7240, Acc=47.62%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1 Epoch 5 Val Acc: 50.00%\n",
            "Best Val Accuracy for Fold 1: 54.55%\n",
            "\n",
            "--- Fold 2/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 2 Epoch 1: 100%|██████████| 22/22 [02:25<00:00,  6.64s/it, Loss=0.7107, Acc=48.24%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 1 Val Acc: 47.62%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 2: 100%|██████████| 22/22 [02:37<00:00,  7.17s/it, Loss=0.7249, Acc=51.76%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 2 Val Acc: 52.38%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 3: 100%|██████████| 22/22 [02:26<00:00,  6.64s/it, Loss=0.7111, Acc=50.59%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 3 Val Acc: 52.38%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 4: 100%|██████████| 22/22 [02:26<00:00,  6.66s/it, Loss=0.8868, Acc=44.71%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 4 Val Acc: 52.38%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 5: 100%|██████████| 22/22 [02:37<00:00,  7.15s/it, Loss=0.5749, Acc=47.06%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 2 Epoch 5 Val Acc: 47.62%\n",
            "Best Val Accuracy for Fold 2: 52.38%\n",
            "\n",
            "--- Fold 3/5 ---\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 3 Epoch 1: 100%|██████████| 22/22 [02:24<00:00,  6.58s/it, Loss=1.1449, Acc=38.82%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 1 Val Acc: 52.38%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 2: 100%|██████████| 22/22 [02:25<00:00,  6.61s/it, Loss=1.1105, Acc=51.76%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 2 Val Acc: 47.62%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 3: 100%|██████████| 22/22 [02:26<00:00,  6.64s/it, Loss=0.7879, Acc=44.71%]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 3 Epoch 3 Val Acc: 38.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 4: 100%|██████████| 22/22 [02:37<00:00,  7.16s/it, Loss=0.8670, Acc=47.06%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 4 Val Acc: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 5: 100%|██████████| 22/22 [02:24<00:00,  6.57s/it, Loss=1.1281, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 5 Val Acc: 47.62%\n",
            "Best Val Accuracy for Fold 3: 52.38%\n",
            "\n",
            "--- Fold 4/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 4 Epoch 1: 100%|██████████| 22/22 [02:25<00:00,  6.60s/it, Loss=0.7759, Acc=54.12%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 1 Val Acc: 57.14%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 2: 100%|██████████| 22/22 [02:24<00:00,  6.57s/it, Loss=0.2796, Acc=57.65%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 2 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 3: 100%|██████████| 22/22 [02:35<00:00,  7.06s/it, Loss=0.5021, Acc=56.47%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 3 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 4: 100%|██████████| 22/22 [02:23<00:00,  6.54s/it, Loss=0.7700, Acc=45.88%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 4 Val Acc: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 5: 100%|██████████| 22/22 [02:25<00:00,  6.61s/it, Loss=0.5962, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 5 Val Acc: 47.62%\n",
            "Best Val Accuracy for Fold 4: 57.14%\n",
            "\n",
            "--- Fold 5/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 5 Epoch 1: 100%|██████████| 22/22 [02:26<00:00,  6.66s/it, Loss=0.7490, Acc=44.71%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 1 Val Acc: 47.62%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 2: 100%|██████████| 22/22 [02:39<00:00,  7.26s/it, Loss=0.5125, Acc=37.65%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 2 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 3: 100%|██████████| 22/22 [02:27<00:00,  6.69s/it, Loss=0.5943, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 3 Val Acc: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 4: 100%|██████████| 22/22 [02:27<00:00,  6.68s/it, Loss=0.3665, Acc=47.06%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 4 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 5: 100%|██████████| 22/22 [02:39<00:00,  7.23s/it, Loss=0.7299, Acc=43.53%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 5 Val Acc: 61.90%\n",
            "Saved new best model for this fold\n",
            "Best Val Accuracy for Fold 5: 61.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import ViTModel, ViTConfig\n",
        "\n",
        "# Configuration\n",
        "FRAME_SIZE = 224\n",
        "CLIP_LENGTH = 4\n",
        "BATCH_SIZE = 8  # Lowered for RAM efficiency\n",
        "EPOCHS = 5     # Use more if you have time/resources\n",
        "NUM_FOLDS = 5\n",
        "SEED = 42\n",
        "\n",
        "class VideoTransform:\n",
        "    def __init__(self, train=True):\n",
        "        self.spatial_transform = A.Compose([\n",
        "            A.RandomResizedCrop(FRAME_SIZE, FRAME_SIZE, scale=(0.2, 1.0)),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.3, contrast=0.3, p=0.7),\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=0.4),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ]) if train else A.Compose([\n",
        "            A.Resize(FRAME_SIZE, FRAME_SIZE),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    def __call__(self, frames):\n",
        "        transformed = []\n",
        "        for frame in frames:\n",
        "            transformed.append(self.spatial_transform(image=frame)['image'])\n",
        "        return torch.stack(transformed)\n",
        "\n",
        "class VideoDeepfakeDataset(Dataset):\n",
        "    def __init__(self, base_path, real_dir=\"videos_real\", fake_dir=\"videos_fake\", transform=None):\n",
        "        self.real_path = os.path.join(base_path, real_dir)\n",
        "        self.fake_path = os.path.join(base_path, fake_dir)\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "        for f in os.listdir(self.real_path):\n",
        "            if f.endswith(\".mp4\"):\n",
        "                self.file_paths.append(os.path.join(self.real_path, f))\n",
        "                self.labels.append(1)\n",
        "        for f in os.listdir(self.fake_path):\n",
        "            if f.endswith(\".mp4\"):\n",
        "                self.file_paths.append(os.path.join(self.fake_path, f))\n",
        "                self.labels.append(0)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "            if len(frames) >= CLIP_LENGTH * 2:\n",
        "                break\n",
        "        cap.release()\n",
        "        clip = self._sample_clip(frames, CLIP_LENGTH)\n",
        "        if self.transform:\n",
        "            clip = self.transform(clip)\n",
        "        return clip, torch.tensor(label)\n",
        "    def _sample_clip(self, frames, clip_length):\n",
        "        if len(frames) < clip_length:\n",
        "            frames += [frames[-1]] * (clip_length - len(frames))\n",
        "        start_idx = np.random.randint(0, max(1, len(frames) - clip_length))\n",
        "        return frames[start_idx : start_idx + clip_length]\n",
        "\n",
        "class MultiScaleLocalAttention(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_channels=32):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 5, padding=2),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 7, padding=3),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(base_channels * 3, base_channels, 1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    def forward(self, x):\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        x_cat = torch.cat([x1, x2, x3], dim=1)\n",
        "        x_fused = self.fuse(x_cat)\n",
        "        pooled = self.pool(x_fused).flatten(1)\n",
        "        return pooled\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, global_dim, local_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(global_dim, out_dim)\n",
        "        self.key_proj = nn.Linear(local_dim, out_dim)\n",
        "        self.value_proj = nn.Linear(local_dim, out_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        Q = self.query_proj(global_feat).unsqueeze(1)\n",
        "        K = self.key_proj(local_feat).unsqueeze(1)\n",
        "        V = self.value_proj(local_feat).unsqueeze(1)\n",
        "        attn_weights = self.softmax(torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(Q.shape[-1]))\n",
        "        fused = torch.bmm(attn_weights, V).squeeze(1)\n",
        "        return fused\n",
        "\n",
        "class HybridFrameFeature(nn.Module):\n",
        "    def __init__(self, vit_ckpt=\"WinKawaks/vit-tiny-patch16-224\", local_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        config = ViTConfig.from_pretrained(vit_ckpt)\n",
        "        self.vit = ViTModel.from_pretrained(vit_ckpt, config=config)\n",
        "        self.local_attn = MultiScaleLocalAttention(in_channels=3, base_channels=local_dim)\n",
        "        self.cross_attn = CrossAttentionFusion(global_dim=config.hidden_size, local_dim=local_dim, out_dim=fusion_dim)\n",
        "        self.out_dim = config.hidden_size + fusion_dim\n",
        "    def forward(self, x):\n",
        "        global_feat = self.vit(x).last_hidden_state[:, 0]\n",
        "        local_feat = self.local_attn(x)\n",
        "        fused = self.cross_attn(global_feat, local_feat)\n",
        "        return torch.cat([global_feat, fused], dim=1)\n",
        "\n",
        "class VideoDeepfakeClassifier(nn.Module):\n",
        "    def __init__(self, frame_model, frame_feat_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.frame_model = frame_model\n",
        "        self.temporal_attn = nn.MultiheadAttention(\n",
        "            embed_dim=frame_feat_dim,\n",
        "            num_heads=4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(frame_feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape[:2]\n",
        "        x = x.view(B * T, *x.shape[2:])\n",
        "        frame_feats = self.frame_model(x)\n",
        "        frame_feats = frame_feats.view(B, T, -1)\n",
        "        attn_out, _ = self.temporal_attn(frame_feats, frame_feats, frame_feats)\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "import random\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# Prepare dataset and labels for splitting\n",
        "base_path = \"/content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD\"\n",
        "dataset = VideoDeepfakeDataset(base_path, transform=VideoTransform(train=True))\n",
        "labels = np.array(dataset.labels)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{NUM_FOLDS} ---\")\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    frame_model = HybridFrameFeature()\n",
        "    frame_feat_dim = frame_model.out_dim\n",
        "    model = VideoDeepfakeClassifier(frame_model, frame_feat_dim).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
        "        for clips, labels in pbar:\n",
        "            clips = clips.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Acc': f\"{100*correct/total:.2f}%\"})\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for clips, labels in val_loader:\n",
        "                clips = clips.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(clips)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"Fold {fold+1} Epoch {epoch+1} Val Acc: {100*val_acc:.2f}%\")\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"best_video_model_hybrid_fold{fold+1}.pth\")\n",
        "            print(\"Saved new best model for this fold\")\n",
        "    print(f\"Best Val Accuracy for Fold {fold+1}: {100*best_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BFQtW1GiRHP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a332379-4e22-4235-f341-4999b3448fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 1 Epoch 1:  18%|█▊        | 2/11 [01:21<05:38, 37.65s/it, Loss=0.9953, Acc=56.25%]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from transformers import ViTModel, ViTConfig\n",
        "import random\n",
        "\n",
        "# Configuration\n",
        "FRAME_SIZE = 224\n",
        "CLIP_LENGTH = 4\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "NUM_FOLDS = 5\n",
        "SEED = 42\n",
        "\n",
        "class VideoTransform:\n",
        "    def __init__(self, train=True):\n",
        "        self.spatial_transform = A.Compose([\n",
        "            A.RandomResizedCrop(FRAME_SIZE, FRAME_SIZE, scale=(0.2, 1.0)),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ColorJitter(brightness=0.3, contrast=0.3, p=0.7),\n",
        "            A.GaussianBlur(blur_limit=(3, 5), p=0.4),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ]) if train else A.Compose([\n",
        "            A.Resize(FRAME_SIZE, FRAME_SIZE),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    def __call__(self, frames):\n",
        "        transformed = []\n",
        "        for frame in frames:\n",
        "            transformed.append(self.spatial_transform(image=frame)['image'])\n",
        "        return torch.stack(transformed)\n",
        "\n",
        "class VideoDeepfakeDataset(Dataset):\n",
        "    def __init__(self, base_path, real_dir=\"videos_real\", fake_dir=\"videos_fake\", transform=None):\n",
        "        self.real_path = os.path.join(base_path, real_dir)\n",
        "        self.fake_path = os.path.join(base_path, fake_dir)\n",
        "        self.file_paths = []\n",
        "        self.labels = []\n",
        "        for f in os.listdir(self.real_path):\n",
        "            if f.endswith(\".mp4\"):\n",
        "                self.file_paths.append(os.path.join(self.real_path, f))\n",
        "                self.labels.append(1)\n",
        "        for f in os.listdir(self.fake_path):\n",
        "            if f.endswith(\".mp4\"):\n",
        "                self.file_paths.append(os.path.join(self.fake_path, f))\n",
        "                self.labels.append(0)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frames.append(frame)\n",
        "            if len(frames) >= CLIP_LENGTH * 2:\n",
        "                break\n",
        "        cap.release()\n",
        "        clip = self._sample_clip(frames, CLIP_LENGTH)\n",
        "        if self.transform:\n",
        "            clip = self.transform(clip)\n",
        "        return clip, torch.tensor(label)\n",
        "    def _sample_clip(self, frames, clip_length):\n",
        "        if len(frames) < clip_length:\n",
        "            frames += [frames[-1]] * (clip_length - len(frames))\n",
        "        start_idx = np.random.randint(0, max(1, len(frames) - clip_length))\n",
        "        return frames[start_idx : start_idx + clip_length]\n",
        "\n",
        "class MultiScaleLocalAttention(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_channels=32):\n",
        "        super().__init__()\n",
        "        self.branch1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 5, padding=2),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.branch3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, base_channels, 7, padding=3),\n",
        "            nn.BatchNorm2d(base_channels),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fuse = nn.Conv2d(base_channels * 3, base_channels, 1)\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    def forward(self, x):\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        x_cat = torch.cat([x1, x2, x3], dim=1)\n",
        "        x_fused = self.fuse(x_cat)\n",
        "        pooled = self.pool(x_fused).flatten(1)\n",
        "        return pooled\n",
        "\n",
        "class CrossAttentionFusion(nn.Module):\n",
        "    def __init__(self, global_dim, local_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(global_dim, out_dim)\n",
        "        self.key_proj = nn.Linear(local_dim, out_dim)\n",
        "        self.value_proj = nn.Linear(local_dim, out_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "    def forward(self, global_feat, local_feat):\n",
        "        Q = self.query_proj(global_feat).unsqueeze(1)\n",
        "        K = self.key_proj(local_feat).unsqueeze(1)\n",
        "        V = self.value_proj(local_feat).unsqueeze(1)\n",
        "        attn_weights = self.softmax(torch.bmm(Q, K.transpose(1, 2)) / np.sqrt(Q.shape[-1]))\n",
        "        fused = torch.bmm(attn_weights, V).squeeze(1)\n",
        "        return fused\n",
        "\n",
        "class HybridFrameFeature(nn.Module):\n",
        "    def __init__(self, vit_ckpt=\"WinKawaks/vit-tiny-patch16-224\", local_dim=32, fusion_dim=64):\n",
        "        super().__init__()\n",
        "        config = ViTConfig.from_pretrained(vit_ckpt)\n",
        "        self.vit = ViTModel.from_pretrained(vit_ckpt, config=config)\n",
        "        self.local_attn = MultiScaleLocalAttention(in_channels=3, base_channels=local_dim)\n",
        "        self.cross_attn = CrossAttentionFusion(global_dim=config.hidden_size, local_dim=local_dim, out_dim=fusion_dim)\n",
        "        self.out_dim = config.hidden_size + fusion_dim\n",
        "    def forward(self, x):\n",
        "        global_feat = self.vit(x).last_hidden_state[:, 0]\n",
        "        local_feat = self.local_attn(x)\n",
        "        fused = self.cross_attn(global_feat, local_feat)\n",
        "        return torch.cat([global_feat, fused], dim=1)\n",
        "\n",
        "class VideoDeepfakeClassifier(nn.Module):\n",
        "    def __init__(self, frame_model, frame_feat_dim, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.frame_model = frame_model\n",
        "        self.temporal_attn = nn.MultiheadAttention(\n",
        "            embed_dim=frame_feat_dim,\n",
        "            num_heads=4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(frame_feat_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape[:2]\n",
        "        x = x.view(B * T, *x.shape[2:])\n",
        "        frame_feats = self.frame_model(x)\n",
        "        frame_feats = frame_feats.view(B, T, -1)\n",
        "        attn_out, _ = self.temporal_attn(frame_feats, frame_feats, frame_feats)\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(SEED)\n",
        "\n",
        "# Prepare dataset and labels for splitting\n",
        "base_path = \"/content/drive/MyDrive/SDFVD Small-scale Deepfake Forgery Video Dataset/SDFVD\"\n",
        "dataset = VideoDeepfakeDataset(base_path, transform=VideoTransform(train=True))\n",
        "labels = np.array(dataset.labels)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels)):\n",
        "    print(f\"\\n--- Fold {fold+1}/{NUM_FOLDS} ---\")\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    frame_model = HybridFrameFeature()\n",
        "    frame_feat_dim = frame_model.out_dim\n",
        "    model = VideoDeepfakeClassifier(frame_model, frame_feat_dim).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Fold {fold+1} Epoch {epoch+1}\")\n",
        "        for clips, labels in pbar:\n",
        "            clips = clips.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(clips)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            pbar.set_postfix({'Loss': f\"{loss.item():.4f}\", 'Acc': f\"{100*correct/total:.2f}%\"})\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for clips, labels in val_loader:\n",
        "                clips = clips.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(clips)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"Fold {fold+1} Epoch {epoch+1} Val Acc: {100*val_acc:.2f}%\")\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), f\"best_video_model_hybrid_fold{fold+1}.pth\")\n",
        "            print(\"Saved new best model for this fold\")\n",
        "    print(f\"Best Val Accuracy for Fold {fold+1}: {100*best_acc:.2f}%\")\n",
        "\n",
        "# --- Small Test Block (for quick evaluation on a few samples) ---\n",
        "# Select a small test subset (e.g., first 8 videos)\n",
        "test_indices = list(range(min(8, len(dataset))))\n",
        "test_subset = Subset(dataset, test_indices)\n",
        "test_loader = DataLoader(\n",
        "    test_subset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Load the best model from the last fold (or specify another fold if you wish)\n",
        "model.load_state_dict(torch.load(f\"best_video_model_hybrid_fold{NUM_FOLDS}.pth\"))\n",
        "model.eval()\n",
        "\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for clips, labels in test_loader:\n",
        "        clips = clips.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(clips)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "        print(f\"Predicted: {predicted.item()}, Actual: {labels.item()}\")\n",
        "\n",
        "test_acc = test_correct / test_total if test_total > 0 else 0\n",
        "print(f\"\\nTest Accuracy on {test_total} samples: {100*test_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "iT5euCgK9WB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3dd010-e0e8-48da-f2e5-77002b8e68d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 1/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 1 Epoch 1: 100%|██████████| 21/21 [02:33<00:00,  7.29s/it, Loss=0.6164, Acc=40.48%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 1 Val Acc: 54.55%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 2: 100%|██████████| 21/21 [02:32<00:00,  7.24s/it, Loss=0.7177, Acc=47.62%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 2 Val Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 3: 100%|██████████| 21/21 [02:33<00:00,  7.32s/it, Loss=0.7064, Acc=41.67%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 3 Val Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 4: 100%|██████████| 21/21 [02:30<00:00,  7.16s/it, Loss=0.7331, Acc=50.00%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 4 Val Acc: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 1 Epoch 5: 100%|██████████| 21/21 [02:25<00:00,  6.91s/it, Loss=0.7240, Acc=47.62%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1 Epoch 5 Val Acc: 50.00%\n",
            "Best Val Accuracy for Fold 1: 54.55%\n",
            "\n",
            "--- Fold 2/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 2 Epoch 1: 100%|██████████| 22/22 [02:27<00:00,  6.69s/it, Loss=0.7107, Acc=48.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 1 Val Acc: 47.62%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 2: 100%|██████████| 22/22 [02:25<00:00,  6.63s/it, Loss=0.7249, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 2 Val Acc: 52.38%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 3: 100%|██████████| 22/22 [02:26<00:00,  6.66s/it, Loss=0.7111, Acc=50.59%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 3 Val Acc: 52.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 4: 100%|██████████| 22/22 [02:25<00:00,  6.61s/it, Loss=0.8868, Acc=44.71%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 4 Val Acc: 52.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 2 Epoch 5: 100%|██████████| 22/22 [02:28<00:00,  6.73s/it, Loss=0.5749, Acc=47.06%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2 Epoch 5 Val Acc: 47.62%\n",
            "Best Val Accuracy for Fold 2: 52.38%\n",
            "\n",
            "--- Fold 3/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 3 Epoch 1: 100%|██████████| 22/22 [02:27<00:00,  6.71s/it, Loss=1.1449, Acc=38.82%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 1 Val Acc: 52.38%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 2: 100%|██████████| 22/22 [02:29<00:00,  6.80s/it, Loss=1.1105, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 2 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 3: 100%|██████████| 22/22 [02:51<00:00,  7.79s/it, Loss=0.7879, Acc=44.71%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 3 Val Acc: 38.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 4: 100%|██████████| 22/22 [02:40<00:00,  7.29s/it, Loss=0.8670, Acc=47.06%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 4 Val Acc: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 3 Epoch 5: 100%|██████████| 22/22 [02:29<00:00,  6.81s/it, Loss=1.1281, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 3 Epoch 5 Val Acc: 47.62%\n",
            "Best Val Accuracy for Fold 3: 52.38%\n",
            "\n",
            "--- Fold 4/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 4 Epoch 1: 100%|██████████| 22/22 [02:27<00:00,  6.71s/it, Loss=0.7759, Acc=54.12%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 1 Val Acc: 57.14%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 2: 100%|██████████| 22/22 [02:26<00:00,  6.64s/it, Loss=0.2796, Acc=57.65%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 2 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 3: 100%|██████████| 22/22 [02:27<00:00,  6.72s/it, Loss=0.5021, Acc=56.47%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 3 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 4: 100%|██████████| 22/22 [02:45<00:00,  7.51s/it, Loss=0.7700, Acc=45.88%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 4 Val Acc: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 4 Epoch 5: 100%|██████████| 22/22 [02:52<00:00,  7.84s/it, Loss=0.5962, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 4 Epoch 5 Val Acc: 47.62%\n",
            "Best Val Accuracy for Fold 4: 57.14%\n",
            "\n",
            "--- Fold 5/5 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Fold 5 Epoch 1: 100%|██████████| 22/22 [02:59<00:00,  8.17s/it, Loss=0.7490, Acc=44.71%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 1 Val Acc: 47.62%\n",
            "Saved new best model for this fold\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 2: 100%|██████████| 22/22 [02:58<00:00,  8.12s/it, Loss=0.5125, Acc=37.65%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 2 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 3: 100%|██████████| 22/22 [02:45<00:00,  7.53s/it, Loss=0.5943, Acc=51.76%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 3 Val Acc: 33.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 4: 100%|██████████| 22/22 [02:43<00:00,  7.43s/it, Loss=0.3665, Acc=47.06%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 4 Val Acc: 47.62%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fold 5 Epoch 5: 100%|██████████| 22/22 [02:52<00:00,  7.83s/it, Loss=0.7299, Acc=43.53%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 5 Epoch 5 Val Acc: 61.90%\n",
            "Saved new best model for this fold\n",
            "Best Val Accuracy for Fold 5: 61.90%\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 0, Actual: 1\n",
            "Predicted: 1, Actual: 1\n",
            "\n",
            "Test Accuracy on 8 samples: 12.50%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "060bc8c3b4e24e09a5fa305c1f4e1a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bbd09b25aa14b5c822c86ed6141e002": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8c8d045a99412a9de2e9dc908c8bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5815d02e630f44d4b16e8833c3314e6e",
            "placeholder": "​",
            "style": "IPY_MODEL_2bbd09b25aa14b5c822c86ed6141e002",
            "value": "Downloading config.json: 100%"
          }
        },
        "5815d02e630f44d4b16e8833c3314e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6074304b84dc4d83a3f811a4b4ac1c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c685cc5c19f4ef08decd16b9e5e4c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_dfb425b6a2484672b0f999962701e205",
            "value": " 23.7M/23.7M [00:00&lt;00:00, 80.4MB/s]"
          }
        },
        "648f506c437541fc9f42c53b9c186854": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f8c8d045a99412a9de2e9dc908c8bb2",
              "IPY_MODEL_8f317aa33a014ce99d00f940f3e9a1c8",
              "IPY_MODEL_8643e24a5e2c49438c0025926eae8fb4"
            ],
            "layout": "IPY_MODEL_e5efee7618484d78b632b2110a4237fa"
          }
        },
        "79748f48d5f74af698ba5490ea0ebe34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa9ffe189d364e38b3fc5bc3b1ea6603",
              "IPY_MODEL_d2af59ae1fae49eb877a7a03e6bb93a9",
              "IPY_MODEL_6074304b84dc4d83a3f811a4b4ac1c04"
            ],
            "layout": "IPY_MODEL_fbd8803c4d174dc2b8a3a8e485b16233"
          }
        },
        "8643e24a5e2c49438c0025926eae8fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8afbca4cd01c413a860bb0a74134da0e",
            "placeholder": "​",
            "style": "IPY_MODEL_ca03098458fb45ffa5049ebda30c08a5",
            "value": " 69.6k/69.6k [00:00&lt;00:00, 2.94MB/s]"
          }
        },
        "86b54ca5ee4447f6a6d31eeb49379f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8afbca4cd01c413a860bb0a74134da0e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c685cc5c19f4ef08decd16b9e5e4c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f317aa33a014ce99d00f940f3e9a1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4e10e9ee2dd46fc898b4834403db5cb",
            "max": 69605,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86b54ca5ee4447f6a6d31eeb49379f0b",
            "value": 69605
          }
        },
        "c4e10e9ee2dd46fc898b4834403db5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6154bc4cc6949caa929c27273eb2ea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca03098458fb45ffa5049ebda30c08a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a43f009e41463db9d4c5d0723139e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2af59ae1fae49eb877a7a03e6bb93a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a43f009e41463db9d4c5d0723139e9",
            "max": 23725127,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6154bc4cc6949caa929c27273eb2ea6",
            "value": 23725127
          }
        },
        "dfb425b6a2484672b0f999962701e205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5efee7618484d78b632b2110a4237fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e92c8506503842ec8fdf4ccee47a9029": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa9ffe189d364e38b3fc5bc3b1ea6603": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e92c8506503842ec8fdf4ccee47a9029",
            "placeholder": "​",
            "style": "IPY_MODEL_060bc8c3b4e24e09a5fa305c1f4e1a3a",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "fbd8803c4d174dc2b8a3a8e485b16233": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}