{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncYd_8qCDISR",
        "outputId": "548fff79-34ed-42f1-e640-86c9d8b9c8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng6coz4CDMt6",
        "outputId": "dd97a3af-a1dd-4520-a06b-96e1c8d1423f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folders in /content/drive/MyDrive/PolyGlotFake3/real3:\n",
            "  - ar\n",
            "  - en\n",
            "  - es\n",
            "  - fr\n",
            "  - ja\n",
            "  - ru\n",
            "  - zh\n",
            "Folders in /content/drive/MyDrive/PolyGlotFake3/fake3:\n",
            "  - to_ar\n",
            "  - to_en\n",
            "  - to_es\n",
            "  - to_fr\n",
            "  - to_ja\n",
            "  - to_ru\n",
            "  - to_zh\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def print_subfolders(directory):\n",
        "  for item in os.listdir(directory):\n",
        "    item_path = os.path.join(directory, item)\n",
        "    if os.path.isdir(item_path):  # Check if it's a directory\n",
        "      print(f\"Folders in {item_path}:\")\n",
        "      for subitem in os.listdir(item_path):\n",
        "        subitem_path = os.path.join(item_path, subitem)\n",
        "        if os.path.isdir(subitem_path):  # Check if it's a subfolder\n",
        "          print(f\"  - {subitem}\")\n",
        "\n",
        "# Start with the main directory\n",
        "print_subfolders('/content/drive/MyDrive/PolyGlotFake3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5YAXkd91Ab",
        "outputId": "57ab6503-5ba5-406d-bdf8-79a38fe809d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of videos in '/content/drive/MyDrive/PolyGlotFake3': 420\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def count_videos(directory):\n",
        "  \"\"\"Counts the number of .mp4 video files in a directory and its subfolders.\"\"\"\n",
        "  video_count = 0\n",
        "  for root, _, files in os.walk(directory):  # Traverse all subdirectories\n",
        "    for file in files:\n",
        "      if file.endswith('.mp4'):\n",
        "        video_count += 1\n",
        "  return video_count\n",
        "\n",
        "# Specify the directory you want to check\n",
        "directory_to_check = '/content/drive/MyDrive/PolyGlotFake3'\n",
        "\n",
        "# Get the video count\n",
        "total_videos = count_videos(directory_to_check)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Total number of videos in '{directory_to_check}': {total_videos}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrsrCTvyWQ52",
        "outputId": "10d7a35b-35ad-46f4-e1fb-5b1cbf277de8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "wCFIMUh_99Nq",
        "outputId": "6bf9379e-5d15-4586-b1f6-91ad83a308b2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-49f1ae269c98>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msoundfile\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWav2Vec2Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWav2Vec2Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhisperFeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWhisperModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1953\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlaceholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1965\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1966\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1967\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1968\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/processing_wav2vec2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mprocessing_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessingKwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProcessorMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAudioInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTokenizedInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTextInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction_wav2vec2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWav2Vec2FeatureExtractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0maudio_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_module_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcustom_object_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m from .image_utils import (\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mChannelDimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mImageInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/image_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_torchvision_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtorchvision_io\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterpolationMode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconvnext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdensenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mefficientnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgooglenet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/convnext.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstochastic_depth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStochasticDepth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_presets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgiou_loss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneralized_box_iou_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmisc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv3dNormActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFrozenBatchNorm2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPermute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSqueezeExcitation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpoolers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultiScaleRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mps_roi_align\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mps_roi_align\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSRoIAlign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mps_roi_pool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mps_roi_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSRoIPool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/poolers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mroi_align\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroi_align\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/ops/roi_align.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_compile_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBroadcastingList2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume_execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlist_backends\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlookup_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregister_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallback_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_compile_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalStateGuard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_compile_pg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_convert\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileContext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCompileId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstructured\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorifyScalarRestartAnalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_guards\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTracingContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/exc.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpytree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/fx/experimental/symbolic_shapes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ordered_set\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOrderedSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_dispatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_traceable_wrapper_subclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m from torch.utils._sympy.functions import (\n\u001b[0m\u001b[1;32m     75\u001b[0m     \u001b[0mApplication\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mCeilToInt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_sympy/functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msympy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msympify\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sympy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mmpmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     raise ImportError(\"SymPy now depends on mpmath as an external library. \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mpmath/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_fp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFPContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_mp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_iv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMPIntervalContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mpmath/ctx_mp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmpmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext_main\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_mpf_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mctx_mp_python\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPythonMPContext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mBaseMPContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mctx_mp_python\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_mpf_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mpmath/ctx_mp_python.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    317\u001b[0m     'raise NotImplementedError(\"complex modulo\")')\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m _mpf.__pow__ = binary_op('__pow__',\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0mmpf_pow_same\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;34m'val = mpf_pow_int(sval, other, prec, rounding)'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreturn_mpf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mpmath/ctx_mp_python.py\u001b[0m in \u001b[0;36mbinary_op\u001b[0;34m(name, with_mpf, with_int, with_mpc)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%NAME%\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mnp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mexec_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mpmath/ctx_mp_python.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model, WhisperFeatureExtractor, WhisperModel\n",
        "\n",
        "# Configuration\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_AUDIO_LENGTH = 30  # seconds\n",
        "MAX_TIME_STEPS = 1500\n",
        "\n",
        "LANG_TO_WAV2VEC2 = {\n",
        "    'en': 'facebook/wav2vec2-large-robust-ft-swbd-300h',\n",
        "    'fr': 'facebook/wav2vec2-large-xlsr-53-french',\n",
        "    'ru': 'anton-l/wav2vec2-large-xlsr-53-russian',\n",
        "    'es': 'facebook/wav2vec2-large-xlsr-53-spanish',\n",
        "    'zh': 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn',\n",
        "    'ja': 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese',\n",
        "    'ar': 'jonatasgrosman/wav2vec2-large-xlsr-53-arabic'\n",
        "}\n",
        "WHISPER_MODEL_ID = \"openai/whisper-small\"\n",
        "\n",
        "def init_processors_and_models(languages):\n",
        "    \"\"\"Initialize models and processors\"\"\"\n",
        "    wav2vec2_processors = {}\n",
        "    wav2vec2_models = {}\n",
        "\n",
        "    for lang in languages:\n",
        "        model_id = LANG_TO_WAV2VEC2[lang]\n",
        "        wav2vec2_processors[lang] = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "        wav2vec2_models[lang] = Wav2Vec2Model.from_pretrained(model_id)\n",
        "\n",
        "    whisper_feature_extractor = WhisperFeatureExtractor.from_pretrained(WHISPER_MODEL_ID)\n",
        "    whisper_model = WhisperModel.from_pretrained(WHISPER_MODEL_ID).encoder\n",
        "\n",
        "    return wav2vec2_processors, wav2vec2_models, whisper_feature_extractor, whisper_model\n",
        "\n",
        "def extract_audio_from_video(video_path):\n",
        "    \"\"\"Extract audio from video files\"\"\"\n",
        "    try:\n",
        "        temp_dir = os.path.join(os.getcwd(), \"temp\")\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        temp_path = os.path.join(temp_dir, f\"temp_{os.path.basename(video_path)}.wav\")\n",
        "\n",
        "        audio = AudioSegment.from_file(video_path)\n",
        "        audio = audio.set_frame_rate(SAMPLE_RATE).set_channels(1)\n",
        "\n",
        "        target_length = MAX_AUDIO_LENGTH * 1000  # milliseconds\n",
        "        if len(audio) < target_length:\n",
        "            silence = AudioSegment.silent(duration=target_length - len(audio))\n",
        "            audio += silence\n",
        "        else:\n",
        "            audio = audio[:target_length]\n",
        "\n",
        "        audio.export(temp_path, format=\"wav\")\n",
        "        return temp_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Video processing error: {str(e)}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if 'audio' in locals():\n",
        "            del audio\n",
        "\n",
        "def extract_audio_features(path, lang, wav2vec2_processors, wav2vec2_models,\n",
        "                          whisper_feature_extractor, whisper_model):\n",
        "    \"\"\"Extract combined audio features with temporal dimension\"\"\"\n",
        "    temp_file = None\n",
        "    try:\n",
        "        if path.lower().endswith('.mp4'):\n",
        "            temp_file = extract_audio_from_video(path)\n",
        "            if not temp_file or not os.path.exists(temp_file):\n",
        "                return None\n",
        "            audio_path = temp_file\n",
        "        else:\n",
        "            audio_path = path\n",
        "\n",
        "        audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
        "        target_samples = SAMPLE_RATE * MAX_AUDIO_LENGTH\n",
        "        if len(audio) < target_samples:\n",
        "            audio = np.pad(audio, (0, target_samples - len(audio)))\n",
        "        else:\n",
        "            audio = audio[:target_samples]\n",
        "\n",
        "        # Wav2Vec2 features (keep temporal dimension)\n",
        "        print(f\"Extracting features using Wav2Vec2 for language: {lang}\")\n",
        "        wav_input = wav2vec2_processors[lang](audio, return_tensors=\"pt\", sampling_rate=SAMPLE_RATE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            wav_features = wav2vec2_models[lang](**wav_input).last_hidden_state\n",
        "            wav_features = wav_features.squeeze(0).numpy()  # Shape: (time_steps, 1024)\n",
        "\n",
        "        print(f\"Wav2Vec2 features shape: {wav_features.shape}\")\n",
        "\n",
        "        # Whisper features (keep temporal dimension)\n",
        "        print(\"Extracting features using Whisper...\")\n",
        "        whisper_input = whisper_feature_extractor(\n",
        "            audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\"\n",
        "        ).input_features\n",
        "\n",
        "        with torch.no_grad():\n",
        "            whisper_features = whisper_model(whisper_input).last_hidden_state\n",
        "            whisper_features = whisper_features.squeeze(0).numpy()  # Shape: (time_steps, 768)\n",
        "\n",
        "        print(f\"Whisper features shape: {whisper_features.shape}\")\n",
        "\n",
        "        # Align time steps\n",
        "        min_time_steps = min(wav_features.shape[0], whisper_features.shape[0])\n",
        "        wav_features = wav_features[:min_time_steps, :]\n",
        "        whisper_features = whisper_features[:min_time_steps, :]\n",
        "\n",
        "        # Combine features\n",
        "        combined_features = np.concatenate([wav_features, whisper_features], axis=1)\n",
        "        print(f\"Combined features shape: {combined_features.shape}\")\n",
        "\n",
        "        # Pad/truncate to fixed length\n",
        "        if combined_features.shape[0] < MAX_TIME_STEPS:\n",
        "            pad = np.zeros((MAX_TIME_STEPS - combined_features.shape[0], 1792))\n",
        "            combined_features = np.vstack([combined_features, pad])\n",
        "        else:\n",
        "            combined_features = combined_features[:MAX_TIME_STEPS]\n",
        "\n",
        "        return combined_features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Feature extraction failed: {str(e)}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            os.remove(temp_file)\n",
        "\n",
        "\n",
        "def process_directory(base_dir, wav2vec2_processors, wav2vec2_models,\n",
        "                     whisper_feature_extractor, whisper_model, is_fake=False):\n",
        "    \"\"\"Process all audio/video files in a directory\"\"\"\n",
        "    features_list = []\n",
        "    labels = []\n",
        "\n",
        "    for folder in os.listdir(base_dir):\n",
        "        folder_path = os.path.join(base_dir, folder)\n",
        "        if not os.path.isdir(folder_path):\n",
        "            continue\n",
        "\n",
        "        lang = folder.replace('to_', '') if is_fake else folder\n",
        "        print(f\"Processing: {folder_path} ({lang})\")\n",
        "\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.lower().endswith('.mp4'):\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                try:\n",
        "                    feature = extract_audio_features(\n",
        "                        file_path, lang,\n",
        "                        wav2vec2_processors, wav2vec2_models,\n",
        "                        whisper_feature_extractor, whisper_model\n",
        "                    )\n",
        "\n",
        "                    if feature is not None:\n",
        "                        # Validate shape\n",
        "                        if feature.shape != (MAX_TIME_STEPS, 1792):\n",
        "                            raise ValueError(f\"Invalid feature shape: {feature.shape}\")\n",
        "                        features_list.append(feature)\n",
        "                        labels.append(1 if is_fake else 0)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed {file_path}: {str(e)}\")\n",
        "\n",
        "    return np.array(features_list), np.array(labels)\n",
        "\n",
        "\n",
        "def save_features_and_labels(features, labels, prefix, save_dir=\"/content/drive/MyDrive/PolyGlotFake3/processed_audio_v1\"):\n",
        "    \"\"\"Save features and labels to .npy files in a dedicated directory\"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Define full paths\n",
        "    features_path = os.path.join(save_dir, f'{prefix}_features1.npy')\n",
        "    labels_path = os.path.join(save_dir, f'{prefix}_labels1.npy')\n",
        "\n",
        "    # Save files\n",
        "    np.save(features_path, features)\n",
        "    np.save(labels_path, labels)\n",
        "    print(f\"Saved to Google Drive:\")\n",
        "    print(f\"- Features: {features_path}\")\n",
        "    print(f\"- Labels: {labels_path}\")\n",
        "\n",
        "# Entry point\n",
        "if __name__ == \"__main__\":\n",
        "    languages = ['en', 'fr', 'ru', 'es', 'zh', 'ja', 'ar']\n",
        "\n",
        "    # Initialize models\n",
        "    wav2vec2_processors, wav2vec2_models, whisper_fe, whisper_model = init_processors_and_models(languages)\n",
        "\n",
        "    # Process real and fake datasets\n",
        "    real_features, real_labels = process_directory(\n",
        "        '/content/drive/MyDrive/PolyGlotFake3/real3',\n",
        "        wav2vec2_processors, wav2vec2_models,\n",
        "        whisper_fe, whisper_model, False\n",
        "    )\n",
        "\n",
        "    fake_features, fake_labels = process_directory(\n",
        "        '/content/drive/MyDrive/PolyGlotFake3/fake3',\n",
        "        wav2vec2_processors, wav2vec2_models,\n",
        "        whisper_fe, whisper_model, True\n",
        "    )\n",
        "\n",
        "        # Usage\n",
        "    save_features_and_labels(real_features, real_labels, 'real_audio1')\n",
        "    save_features_and_labels(fake_features, fake_labels, 'fake_audio1')\n",
        "\n",
        "    print(f\"Real dataset shape: {real_features.shape}\")\n",
        "    print(f\"Fake dataset shape: {fake_features.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        },
        "id": "GLEn_QVv79ZO",
        "outputId": "b0d630a8-2714-4b5d-e518-1c5ed80cdb3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe==0.10.10\n",
            "  Downloading mediapipe-0.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (0.5.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (2.0.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.10) (4.11.0.86)\n",
            "Collecting protobuf<4,>=3.11 (from mediapipe==0.10.10)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.10)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.10) (1.17.1)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.10) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.10) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.10) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.10) (1.15.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.10) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.10) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.10) (1.17.0)\n",
            "Downloading mediapipe-0.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: protobuf, sounddevice, mediapipe\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.10 protobuf-3.20.3 sounddevice-0.5.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "56fd14c2ca5f4681afe1f2056c92acc5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8puVDlKrBHU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52dd92a0-89b0-4cd1-8963-c3ed02747b4b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Found 140 total videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing real (ru): 100%|| 10/10 [01:43<00:00, 10.32s/it]\n",
            "Processing real (en): 100%|| 10/10 [01:24<00:00,  8.46s/it]\n",
            "Processing real (ja): 100%|| 10/10 [01:32<00:00,  9.23s/it]\n",
            "Processing real (fr): 100%|| 10/10 [01:19<00:00,  7.99s/it]\n",
            "Processing real (es): 100%|| 10/10 [01:42<00:00, 10.23s/it]\n",
            "Processing real (zh): 100%|| 10/10 [01:08<00:00,  6.81s/it]\n",
            "Processing real (ar): 100%|| 10/10 [01:22<00:00,  8.28s/it]\n",
            "Processing fake (to_zh): 100%|| 10/10 [02:51<00:00, 17.19s/it]\n",
            "Processing fake (to_ru): 100%|| 10/10 [02:56<00:00, 17.62s/it]\n",
            "Processing fake (to_ja): 100%|| 10/10 [02:34<00:00, 15.40s/it]\n",
            "Processing fake (to_fr): 100%|| 10/10 [02:45<00:00, 16.54s/it]\n",
            "Processing fake (to_es): 100%|| 10/10 [02:47<00:00, 16.73s/it]\n",
            "Processing fake (to_ar): 100%|| 10/10 [01:56<00:00, 11.60s/it]\n",
            "Processing fake (to_en): 100%|| 10/10 [02:45<00:00, 16.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Processing Report:\n",
            " Successfully processed: 140/140\n",
            " Skipped: 0/140\n",
            "\n",
            " Successfully processed all videos!\n",
            " Saved video processing results (version 1):\n",
            "   - Features shape: (140, 40, 64, 64, 3) -> /content/drive/MyDrive/PolyGlotFake2/processed_data/lip_features_features_v1.npy\n",
            "   - Labels shape: (140,) -> /content/drive/MyDrive/PolyGlotFake2/processed_data/lip_features_labels_v1.npy\n",
            "   - Total samples: 140 (70 fake, 70 real)\n",
            " Saved video processing results (version 1):\n",
            "   - Features shape: (112, 40, 64, 64, 3) -> /content/drive/MyDrive/PolyGlotFake2/processed_data/train_features_v1.npy\n",
            "   - Labels shape: (112,) -> /content/drive/MyDrive/PolyGlotFake2/processed_data/train_labels_v1.npy\n",
            "   - Total samples: 112 (56 fake, 56 real)\n",
            " Saved video processing results (version 1):\n",
            "   - Features shape: (28, 40, 64, 64, 3) -> /content/drive/MyDrive/PolyGlotFake2/processed_data/test_features_v1.npy\n",
            "   - Labels shape: (28,) -> /content/drive/MyDrive/PolyGlotFake2/processed_data/test_labels_v1.npy\n",
            "   - Total samples: 28 (14 fake, 14 real)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize MediaPipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "# MediaPipe lips indices (upper and lower lips)\n",
        "LIPS_INDICES = [\n",
        "    # Upper outer lip\n",
        "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291,\n",
        "    # Lower outer lip\n",
        "    375, 321, 405, 314, 17, 84, 181, 91, 146,\n",
        "    # Upper inner lip\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308,\n",
        "    # Lower inner lip\n",
        "    324, 318, 402, 317, 14, 87, 178, 88, 95\n",
        "]\n",
        "\n",
        "def initialize_face_mesh():\n",
        "    \"\"\"Initialize MediaPipe Face Mesh with optimized parameters\"\"\"\n",
        "    return mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        refine_landmarks=True,\n",
        "        min_detection_confidence=0.3,\n",
        "        min_tracking_confidence=0.3\n",
        "    )\n",
        "\n",
        "def detect_and_crop_face(image, face_mesh, debug=False):\n",
        "    \"\"\"Detect face in image and return bounding box with landmarks\"\"\"\n",
        "    if len(image.shape) == 2 or image.shape[2] == 1:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "    if not results.multi_face_landmarks:\n",
        "        return None\n",
        "\n",
        "    face_landmarks = results.multi_face_landmarks[0]\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    # Calculate face bounding box\n",
        "    x_coords = [int(lm.x * w) for lm in face_landmarks.landmark]\n",
        "    y_coords = [int(lm.y * h) for lm in face_landmarks.landmark]\n",
        "\n",
        "    x_min, x_max = max(0, min(x_coords)), min(w, max(x_coords))\n",
        "    y_min, y_max = max(0, min(y_coords)), min(h, max(y_coords))\n",
        "\n",
        "    # Add 10% margin\n",
        "    margin = int(0.1 * max(x_max - x_min, y_max - y_min))\n",
        "    return (\n",
        "        max(x_min - margin, 0),\n",
        "        max(y_min - margin, 0),\n",
        "        min(x_max + margin, w) - max(x_min - margin, 0),\n",
        "        min(y_max + margin, h) - max(y_min - margin, 0)\n",
        "    ), face_landmarks\n",
        "\n",
        "def extract_lip_region(image, face_mesh, debug=False):\n",
        "    \"\"\"Extract lip region from image using Face Mesh\"\"\"\n",
        "    result = detect_and_crop_face(image, face_mesh, debug)\n",
        "    if not result:\n",
        "        return None\n",
        "\n",
        "    (x, y, w, h), face_landmarks = result\n",
        "    h_img, w_img = image.shape[:2]\n",
        "\n",
        "    # Extract lip landmarks\n",
        "    lip_points = []\n",
        "    for idx in LIPS_INDICES:\n",
        "        lm = face_landmarks.landmark[idx]\n",
        "        lip_points.append((int(lm.x * w_img), int(lm.y * h_img)))\n",
        "\n",
        "    # Calculate lip bounding box with margin\n",
        "    x_coords, y_coords = zip(*lip_points)\n",
        "    x_min, x_max = max(0, min(x_coords)), min(w_img, max(x_coords))\n",
        "    y_min, y_max = max(0, min(y_coords)), min(h_img, max(y_coords))\n",
        "\n",
        "    margin = int(0.2 * max(x_max - x_min, y_max - y_min))\n",
        "    return image[\n",
        "        max(y_min - margin, 0):min(y_max + margin, h_img),\n",
        "        max(x_min - margin, 0):min(x_max + margin, w_img)\n",
        "    ]\n",
        "\n",
        "def process_video(video_path, face_mesh, target_frames=40, debug=False):\n",
        "    \"\"\"Process video and extract lip regions\"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return None\n",
        "\n",
        "    frames = []\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_interval = max(1, int(cap.get(cv2.CAP_PROP_FPS) // 5))\n",
        "\n",
        "    for frame_idx in range(0, total_frames, frame_interval):\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        lip_region = extract_lip_region(frame, face_mesh, debug)\n",
        "        if lip_region is not None and lip_region.size > 0:\n",
        "            try:\n",
        "                resized = cv2.resize(lip_region, (64, 64))\n",
        "                frames.append(resized)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        if len(frames) >= target_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Pad if insufficient frames\n",
        "    if len(frames) < target_frames:\n",
        "        padding = [np.zeros((64, 64, 3), dtype=np.uint8)] * (target_frames - len(frames))\n",
        "        frames.extend(padding)\n",
        "\n",
        "    return np.array(frames[:target_frames], dtype=np.float32) / 255.0\n",
        "\n",
        "def load_dataset(root_dir, debug=True):\n",
        "    \"\"\"Load dataset with comprehensive tracking\"\"\"\n",
        "    face_mesh = initialize_face_mesh()\n",
        "    X, y = [], []\n",
        "    stats = {\n",
        "        'total': 0,\n",
        "        'processed': 0,\n",
        "        'skipped': 0,\n",
        "        'reasons': {},\n",
        "        'skipped_files': []\n",
        "    }\n",
        "\n",
        "    # First count total videos\n",
        "    for label, folders in [('real', 'real2'), ('fake', 'fake2')]:\n",
        "        label_path = os.path.join(root_dir, folders)\n",
        "        if os.path.exists(label_path):\n",
        "            for lang in os.listdir(label_path):\n",
        "                lang_path = os.path.join(label_path, lang)\n",
        "                if os.path.isdir(lang_path):\n",
        "                    stats['total'] += len([\n",
        "                        f for f in os.listdir(lang_path)\n",
        "                        if f.lower().endswith('.mp4')\n",
        "                    ])\n",
        "\n",
        "    print(f\" Found {stats['total']} total videos\")\n",
        "\n",
        "    # Process videos\n",
        "    for label, (label_name, label_dir) in enumerate([('real', 'real2'), ('fake', 'fake2')]):\n",
        "        label_path = os.path.join(root_dir, label_dir)\n",
        "        if not os.path.exists(label_path):\n",
        "            print(f\" Missing directory: {label_path}\")\n",
        "            stats['reasons']['missing_directory'] = stats['reasons'].get('missing_directory', 0) + 1\n",
        "            continue\n",
        "\n",
        "        for lang in os.listdir(label_path):\n",
        "            lang_path = os.path.join(label_path, lang)\n",
        "            if not os.path.isdir(lang_path):\n",
        "                continue\n",
        "\n",
        "            video_files = [f for f in os.listdir(lang_path)\n",
        "                          if f.lower().endswith('.mp4')]\n",
        "\n",
        "            for video_file in tqdm(video_files, desc=f\"Processing {label_name} ({lang})\"):\n",
        "                video_path = os.path.join(lang_path, video_file)\n",
        "                try:\n",
        "                    frames = process_video(video_path, face_mesh, debug=debug)\n",
        "                    if frames is not None and len(frames) > 0:\n",
        "                        X.append(frames)\n",
        "                        y.append(label)\n",
        "                        stats['processed'] += 1\n",
        "                    else:\n",
        "                        stats['skipped'] += 1\n",
        "                        stats['reasons']['no_lip_frames'] = stats['reasons'].get('no_lip_frames', 0) + 1\n",
        "                        stats['skipped_files'].append(video_path)\n",
        "                except Exception as e:\n",
        "                    stats['skipped'] += 1\n",
        "                    stats['reasons'][str(type(e).__name__)] = stats['reasons'].get(str(type(e).__name__), 0) + 1\n",
        "                    stats['skipped_files'].append(video_path)\n",
        "\n",
        "    face_mesh.close()\n",
        "\n",
        "    # Print final report\n",
        "    print(\"\\n Processing Report:\")\n",
        "    print(f\" Successfully processed: {stats['processed']}/{stats['total']}\")\n",
        "    print(f\" Skipped: {stats['skipped']}/{stats['total']}\")\n",
        "\n",
        "    if stats['skipped'] > 0:\n",
        "        print(\"\\n Skip Reasons:\")\n",
        "        for reason, count in stats['reasons'].items():\n",
        "            print(f\"- {reason}: {count}\")\n",
        "\n",
        "        print(\"\\n Top 5 Skipped Files:\")\n",
        "        for idx, f in enumerate(stats['skipped_files'][:5], 1):\n",
        "            print(f\"{idx}. {f}\")\n",
        "        if len(stats['skipped_files']) > 5:\n",
        "            print(f\"... and {len(stats['skipped_files'])-5} more\")\n",
        "\n",
        "    if stats['processed'] + stats['skipped'] == stats['total']:\n",
        "        print(\"\\n Successfully processed all videos!\")\n",
        "    else:\n",
        "        missing = stats['total'] - (stats['processed'] + stats['skipped'])\n",
        "        print(f\"\\n Warning: {missing} videos unaccounted for - check file formats\")\n",
        "\n",
        "    return np.array(X, dtype=object), np.array(y)\n",
        "\n",
        "def save_features_and_labels(features, labels, prefix, save_dir=\"/content/drive/MyDrive/PolyGlotFake2/processed_video\"):\n",
        "    \"\"\"Save features and labels to .npy files in a dedicated directory with versioning support.\n",
        "\n",
        "    Args:\n",
        "        features (np.array): Processed video frames or features\n",
        "        labels (np.array): Corresponding labels\n",
        "        prefix (str): Prefix for filenames (e.g., 'train', 'val', 'test')\n",
        "        save_dir (str): Directory to save files (defaults to Google Drive)\n",
        "    \"\"\"\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Versioning support - finds next available version number\n",
        "    version = 0\n",
        "    while True:\n",
        "        features_path = os.path.join(save_dir, f'{prefix}_features_v{version}.npy')\n",
        "        labels_path = os.path.join(save_dir, f'{prefix}_labels_v{version}.npy')\n",
        "        if not (os.path.exists(features_path) or os.path.exists(labels_path)):\n",
        "            break\n",
        "        version += 1\n",
        "\n",
        "    # Save files with metadata in filename\n",
        "    np.save(features_path, features)\n",
        "    np.save(labels_path, labels)\n",
        "\n",
        "    print(f\" Saved video processing results (version {version}):\")\n",
        "    print(f\"   - Features shape: {features.shape} -> {features_path}\")\n",
        "    print(f\"   - Labels shape: {labels.shape} -> {labels_path}\")\n",
        "    print(f\"   - Total samples: {len(labels)} ({np.sum(labels==1)} fake, {np.sum(labels==0)} real)\")\n",
        "\n",
        "def load_features_and_labels(prefix, version=None, save_dir=\"/content/drive/MyDrive/PolyGlotFake2/processed_video\"):\n",
        "    \"\"\"Load saved features and labels from directory.\n",
        "\n",
        "    Args:\n",
        "        prefix (str): Prefix used when saving (e.g., 'train', 'val', 'test')\n",
        "        version (int/str): Specific version to load ('latest' or number)\n",
        "        save_dir (str): Directory where files are saved\n",
        "\n",
        "    Returns:\n",
        "        tuple: (features, labels) as numpy arrays\n",
        "    \"\"\"\n",
        "    if version == 'latest':\n",
        "        # Find all matching files and get highest version\n",
        "        existing_files = [f for f in os.listdir(save_dir)\n",
        "                         if f.startswith(f'{prefix}_features_v')]\n",
        "        if not existing_files:\n",
        "            raise FileNotFoundError(f\"No files found with prefix '{prefix}'\")\n",
        "        versions = [int(f.split('_v')[1].split('.npy')[0]) for f in existing_files]\n",
        "        version = max(versions)\n",
        "\n",
        "    features_path = os.path.join(save_dir, f'{prefix}_features_v{version}.npy')\n",
        "    labels_path = os.path.join(save_dir, f'{prefix}_labels_v{version}.npy')\n",
        "\n",
        "    if not (os.path.exists(features_path) and os.path.exists(labels_path)):\n",
        "        raise FileNotFoundError(f\"Couldn't find both feature and label files for version {version}\")\n",
        "\n",
        "    features = np.load(features_path, allow_pickle=True)\n",
        "    labels = np.load(labels_path)\n",
        "\n",
        "    print(f\" Loaded version {version}:\")\n",
        "    print(f\"   - Features shape: {features.shape}\")\n",
        "    print(f\"   - Labels shape: {labels.shape}\")\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "def save_dataset(X, y, output_dir=\"processed_data\"):\n",
        "    \"\"\"Legacy function for backward compatibility\"\"\"\n",
        "    print(\" Note: save_dataset() is deprecated. Use save_features_and_labels() instead.\")\n",
        "    save_features_and_labels(X, y, 'full_dataset', output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/PolyGlotFake2\"\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/PolyGlotFake2/processed_data\"\n",
        "\n",
        "    # Process dataset\n",
        "    X, y = load_dataset(DATASET_PATH)\n",
        "\n",
        "    # Save processed data\n",
        "    if len(X) > 0:\n",
        "        # Using the new saving function\n",
        "        save_features_and_labels(X, y, 'lip_features', OUTPUT_DIR)\n",
        "\n",
        "        # Can also save train/test splits if needed\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "        save_features_and_labels(X_train, y_train, 'train', OUTPUT_DIR)\n",
        "        save_features_and_labels(X_test, y_test, 'test', OUTPUT_DIR)\n",
        "    else:\n",
        "        print(\" No data to save - processing failed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, LSTM, Dense, Dropout, TimeDistributed, Flatten, concatenate\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from google.colab import drive\n",
        "import gc\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define constants\n",
        "MAX_TIME_STEPS = 1500\n",
        "BATCH_SIZE = 4  # Adjust based on your RAM\n",
        "NUM_EPOCHS = 30\n",
        "\n",
        "# Updated Generator Class with proper output specification\n",
        "class MultimodalDataGenerator(Sequence):\n",
        "    def __init__(self, video_data, audio_data, labels, batch_size):\n",
        "        self.video_data = video_data\n",
        "        self.audio_data = audio_data\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(self.labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "        batch_video = self.video_data[batch_indices].astype('float32')\n",
        "        batch_audio = self.audio_data[batch_indices].astype('float32')\n",
        "        batch_labels = self.labels[batch_indices].astype('float32')\n",
        "\n",
        "        # Instead of returning [input1, input2], label\n",
        "        # Return a tuple of ({input1, input2}, label)\n",
        "        return {\"input_1\": batch_video, \"input_2\": batch_audio}, batch_labels\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        np.random.shuffle(self.indices)\n",
        "\n",
        "# Load and process data\n",
        "def load_and_process_data():\n",
        "    print(\"Loading audio features...\")\n",
        "    real_audio = np.load('/content/drive/MyDrive/PolyGlotFake2/processed_audio_v1/real_audio1_features1.npy')\n",
        "    fake_audio = np.load('/content/drive/MyDrive/PolyGlotFake2/processed_audio_v1/fake_audio1_features1.npy')\n",
        "\n",
        "    print(\"Loading visual features...\")\n",
        "    visual_data = np.load('/content/drive/MyDrive/PolyGlotFake2/processed_data/lip_features_features_v0.npy', allow_pickle=True)\n",
        "    visual_labels = np.load('/content/drive/MyDrive/PolyGlotFake2/processed_data/lip_features_labels_v0.npy', allow_pickle=True)\n",
        "\n",
        "    # Combine and split data\n",
        "    print(\"Combining features...\")\n",
        "    all_audio = np.concatenate([real_audio, fake_audio])\n",
        "    all_labels = np.concatenate([np.zeros(len(real_audio)), np.ones(len(fake_audio))])\n",
        "\n",
        "    # Free memory\n",
        "    del real_audio, fake_audio\n",
        "    gc.collect()\n",
        "\n",
        "    assert len(all_audio) == len(visual_data), \"Data length mismatch\"\n",
        "    assert np.array_equal(all_labels, visual_labels), \"Label mismatch\"\n",
        "\n",
        "    # Split data\n",
        "    print(\"Splitting data...\")\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        np.arange(len(all_labels)),\n",
        "        test_size=0.2,\n",
        "        stratify=all_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        visual_data[train_idx],\n",
        "        all_audio[train_idx],\n",
        "        all_labels[train_idx],\n",
        "        visual_data[test_idx],\n",
        "        all_audio[test_idx],\n",
        "        all_labels[test_idx]\n",
        "    )\n",
        "\n",
        "# Load data\n",
        "video_train, audio_train, y_train, video_test, audio_test, y_test = load_and_process_data()\n",
        "\n",
        "# Create generators\n",
        "train_generator = MultimodalDataGenerator(video_train, audio_train, y_train, BATCH_SIZE)\n",
        "test_generator = MultimodalDataGenerator(video_test, audio_test, y_test, BATCH_SIZE)\n",
        "\n",
        "# Model definition\n",
        "def create_multimodal_model():\n",
        "    # Video branch - giving names to inputs for clarity\n",
        "    video_input = Input(shape=video_train.shape[1:], name='input_1')\n",
        "    x = TimeDistributed(Conv2D(16, (3,3), activation='relu'))(video_input)\n",
        "    x = TimeDistributed(MaxPooling2D(2,2))(x)\n",
        "    x = TimeDistributed(Conv2D(32, (3,3), activation='relu'))(x)\n",
        "    x = TimeDistributed(MaxPooling2D(2,2))(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = LSTM(64)(x)\n",
        "    video_branch = Model(video_input, x)\n",
        "\n",
        "    # Audio branch - giving names to inputs for clarity\n",
        "    audio_input = Input(shape=(MAX_TIME_STEPS, 1792), name='input_2')\n",
        "    y = LSTM(128)(audio_input)\n",
        "    y = Dense(64, activation='relu')(y)\n",
        "    audio_branch = Model(audio_input, y)\n",
        "\n",
        "    # Combined model\n",
        "    combined = concatenate([video_branch.output, audio_branch.output])\n",
        "    z = Dense(32, activation='relu')(combined)\n",
        "    z = Dropout(0.3)(z)\n",
        "    z = Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "    model = Model(inputs=[video_branch.input, audio_branch.input], outputs=z)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.Precision(name='precision'),\n",
        "                tf.keras.metrics.Recall(name='recall'), tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Create model\n",
        "model = create_multimodal_model()\n",
        "model.summary()\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_model.h5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_accuracy'\n",
        ")\n",
        "\n",
        "# Train model with modified approach\n",
        "print(\"Training model...\")\n",
        "\n",
        "# Option 1: Use the fit method with custom generators\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=test_generator,\n",
        "    epochs=NUM_EPOCHS,\n",
        "    callbacks=[early_stopping, checkpoint],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\nEvaluating model...\")\n",
        "y_pred = model.predict(test_generator)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes, target_names=['Real', 'Fake']))\n",
        "\n",
        "# Save final model\n",
        "model.save('polyglot_fake_detector.h5')\n",
        "print(\"Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bhFL-9-0pJet",
        "outputId": "580d63c0-9dcf-4c67-e580-27e2e1f4b731"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Loading audio features...\n",
            "Loading visual features...\n",
            "Combining features...\n",
            "Splitting data...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " input_1              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                                           \n",
              "\n",
              " time_distributed     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span>  input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                                          \n",
              "\n",
              " time_distributed_1   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                                          \n",
              "\n",
              " time_distributed_2   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>,          <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                          \n",
              "\n",
              " time_distributed_3   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                          \n",
              "\n",
              " input_2              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>,                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                                            \n",
              "\n",
              " time_distributed_4   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                                                     \n",
              "\n",
              " lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">983,552</span>  input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,622,272</span>  time_distributed \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span>  lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              "\n",
              " concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],       \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,128</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              " dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " input_1              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m,              \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)                                           \n",
              "\n",
              " time_distributed     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m62\u001b[0m,            \u001b[38;5;34m448\u001b[0m  input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m16\u001b[0m)                                          \n",
              "\n",
              " time_distributed_1   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m31\u001b[0m,              \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m16\u001b[0m)                                          \n",
              "\n",
              " time_distributed_2   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m29\u001b[0m,          \u001b[38;5;34m4,640\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                          \n",
              "\n",
              " time_distributed_3   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m14\u001b[0m,              \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                          \n",
              "\n",
              " input_2              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m,                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m1792\u001b[0m)                                            \n",
              "\n",
              " time_distributed_4   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m6272\u001b[0m)            \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)                                                     \n",
              "\n",
              " lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           \u001b[38;5;34m983,552\u001b[0m  input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " lstm (\u001b[38;5;33mLSTM\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)          \u001b[38;5;34m1,622,272\u001b[0m  time_distributed \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)              \u001b[38;5;34m8,256\u001b[0m  lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              "\n",
              " concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],       \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                       dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m4,128\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " dropout (\u001b[38;5;33mDropout\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n",
              " dense_2 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                  \u001b[38;5;34m33\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,623,329</span> (10.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,623,329\u001b[0m (10.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,623,329</span> (10.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,623,329\u001b[0m (10.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - accuracy: 0.6631 - auc: 0.6820 - loss: 0.6625 - precision: 0.6949 - recall: 0.7054"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 6s/step - accuracy: 0.6633 - auc: 0.6825 - loss: 0.6616 - precision: 0.6944 - recall: 0.7033 - val_accuracy: 0.7143 - val_auc: 0.9031 - val_loss: 0.5434 - val_precision: 0.8000 - val_recall: 0.5714\n",
            "Epoch 2/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 6s/step - accuracy: 0.7037 - auc: 0.7348 - loss: 0.5809 - precision: 0.7123 - recall: 0.6973 - val_accuracy: 0.6071 - val_auc: 0.9107 - val_loss: 0.6045 - val_precision: 0.8000 - val_recall: 0.2857\n",
            "Epoch 3/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8374 - auc: 0.9340 - loss: 0.3912 - precision: 0.8796 - recall: 0.8390"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 6s/step - accuracy: 0.8380 - auc: 0.9336 - loss: 0.3911 - precision: 0.8793 - recall: 0.8390 - val_accuracy: 0.8214 - val_auc: 0.9464 - val_loss: 0.3772 - val_precision: 0.9091 - val_recall: 0.7143\n",
            "Epoch 4/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.8357 - auc: 0.9389 - loss: 0.3593 - precision: 0.7956 - recall: 0.9023"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 6s/step - accuracy: 0.8355 - auc: 0.9383 - loss: 0.3595 - precision: 0.7962 - recall: 0.9007 - val_accuracy: 0.8571 - val_auc: 0.9694 - val_loss: 0.2734 - val_precision: 0.9167 - val_recall: 0.7857\n",
            "Epoch 5/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 6s/step - accuracy: 0.9219 - auc: 0.9428 - loss: 0.2189 - precision: 0.9470 - recall: 0.9027 - val_accuracy: 0.8571 - val_auc: 0.9668 - val_loss: 0.2796 - val_precision: 1.0000 - val_recall: 0.7143\n",
            "Epoch 6/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - accuracy: 0.9633 - auc: 0.9779 - loss: 0.1957 - precision: 0.9631 - recall: 0.9578"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 6s/step - accuracy: 0.9624 - auc: 0.9781 - loss: 0.1955 - precision: 0.9619 - recall: 0.9574 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.1354 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 7/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 6s/step - accuracy: 0.8857 - auc: 0.9594 - loss: 0.2565 - precision: 0.8607 - recall: 0.8940 - val_accuracy: 0.8929 - val_auc: 0.9847 - val_loss: 0.2151 - val_precision: 1.0000 - val_recall: 0.7857\n",
            "Epoch 8/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 6s/step - accuracy: 0.9056 - auc: 0.9801 - loss: 0.2183 - precision: 0.9282 - recall: 0.8136 - val_accuracy: 0.8571 - val_auc: 0.9643 - val_loss: 0.3229 - val_precision: 1.0000 - val_recall: 0.7143\n",
            "Epoch 9/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 6s/step - accuracy: 0.8965 - auc: 0.9729 - loss: 0.2004 - precision: 0.9114 - recall: 0.8798 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0972 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 10/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m155s\u001b[0m 6s/step - accuracy: 0.9521 - auc: 0.9954 - loss: 0.1277 - precision: 0.9167 - recall: 0.9830 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.1744 - val_precision: 0.9333 - val_recall: 1.0000\n",
            "Epoch 11/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 6s/step - accuracy: 0.9117 - auc: 0.9846 - loss: 0.1915 - precision: 0.8911 - recall: 0.9182 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.1058 - val_precision: 1.0000 - val_recall: 0.9286\n",
            "Epoch 12/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 6s/step - accuracy: 0.9909 - auc: 0.9956 - loss: 0.0857 - precision: 0.9904 - recall: 0.9905 - val_accuracy: 0.9286 - val_auc: 0.9949 - val_loss: 0.1516 - val_precision: 1.0000 - val_recall: 0.8571\n",
            "Epoch 13/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 6s/step - accuracy: 0.9355 - auc: 0.9910 - loss: 0.1356 - precision: 0.9435 - recall: 0.9291 - val_accuracy: 0.9286 - val_auc: 0.9898 - val_loss: 0.1258 - val_precision: 0.9286 - val_recall: 0.9286\n",
            "Epoch 14/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 6s/step - accuracy: 0.9382 - auc: 0.9826 - loss: 0.1726 - precision: 0.9104 - recall: 0.9510 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.0701 - val_precision: 0.9333 - val_recall: 1.0000\n",
            "Epoch 15/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 6s/step - accuracy: 0.9887 - auc: 0.9987 - loss: 0.0649 - precision: 0.9869 - recall: 0.9911 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0539 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 16/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 6s/step - accuracy: 0.9994 - auc: 0.9652 - loss: 0.0531 - precision: 0.9655 - recall: 0.9643 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0444 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 17/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 5s/step - accuracy: 0.9977 - auc: 1.0000 - loss: 0.0192 - precision: 1.0000 - recall: 0.9954 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.0788 - val_precision: 0.9333 - val_recall: 1.0000\n",
            "Epoch 18/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 6s/step - accuracy: 0.9933 - auc: 0.9998 - loss: 0.0228 - precision: 1.0000 - recall: 0.9859 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.0653 - val_precision: 0.9333 - val_recall: 1.0000\n",
            "Epoch 19/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 6s/step - accuracy: 0.9868 - auc: 0.9984 - loss: 0.0418 - precision: 0.9865 - recall: 0.9875 - val_accuracy: 0.8571 - val_auc: 0.9694 - val_loss: 0.3593 - val_precision: 1.0000 - val_recall: 0.7143\n",
            "Epoch 20/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 6s/step - accuracy: 0.9668 - auc: 0.9922 - loss: 0.1348 - precision: 0.9783 - recall: 0.9544 - val_accuracy: 0.9286 - val_auc: 0.9949 - val_loss: 0.1006 - val_precision: 0.9286 - val_recall: 0.9286\n",
            "Epoch 21/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 6s/step - accuracy: 0.9703 - auc: 0.9976 - loss: 0.0776 - precision: 0.9612 - recall: 0.9814 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0363 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 22/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 6s/step - accuracy: 0.9512 - auc: 0.9862 - loss: 0.1341 - precision: 0.9521 - recall: 0.9621 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.0984 - val_precision: 1.0000 - val_recall: 0.9286\n",
            "Epoch 23/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0303 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0582 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 24/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0309 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0509 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 25/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0272 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 0.9643 - val_auc: 1.0000 - val_loss: 0.0444 - val_precision: 0.9333 - val_recall: 1.0000\n",
            "Epoch 26/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0083 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0149 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 27/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0033 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0144 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 28/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0031 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0098 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 29/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.0017 - precision: 1.0000 - recall: 1.0000 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0081 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "Epoch 30/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 6s/step - accuracy: 1.0000 - auc: 0.9655 - loss: 0.0016 - precision: 0.9655 - recall: 0.9655 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.0070 - val_precision: 1.0000 - val_recall: 1.0000\n",
            "\n",
            "Evaluating model...\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 992ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.43      0.43      0.43        14\n",
            "        Fake       0.43      0.43      0.43        14\n",
            "\n",
            "    accuracy                           0.43        28\n",
            "   macro avg       0.43      0.43      0.43        28\n",
            "weighted avg       0.43      0.43      0.43        28\n",
            "\n",
            "Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 2\n",
        "SEQ_LENGTH = 40\n",
        "FRAME_SIZE = 64\n",
        "MAX_TIME_STEPS = 1500\n",
        "AUDIO_FEAT_DIM = 1792\n",
        "\n",
        "# Path configuration\n",
        "PATHS = {\n",
        "    'train_video': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_features_v1.npy',\n",
        "    'train_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_labels_v1.npy',\n",
        "    'test_video': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_features_v1.npy',\n",
        "    'test_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_labels_v1.npy',\n",
        "    'real_audio': '/content/drive/MyDrive/PolyGlotFake2/processed_audio_v1/real_audio1_features1.npy',\n",
        "    'fake_audio': '/content/drive/MyDrive/PolyGlotFake2/processed_audio_v1/fake_audio1_features1.npy'\n",
        "}\n",
        "\n",
        "class SafeDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, video_data, audio_data, labels, batch_size):\n",
        "        self.video_data = video_data\n",
        "        self.audio_data = audio_data\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.indices = np.arange(len(labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "        video_batch = self.video_data[batch_indices].astype('float32') / 255.0\n",
        "        audio_batch = self.audio_data[batch_indices].astype('float32')\n",
        "        labels_batch = self.labels[batch_indices].astype('float32')\n",
        "\n",
        "        return {'video_input': video_batch, 'audio_input': audio_batch}, labels_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indices = np.random.permutation(self.indices)\n",
        "\n",
        "def create_optimized_model():\n",
        "    # Video pathway\n",
        "    video_input = layers.Input(\n",
        "        shape=(SEQ_LENGTH, FRAME_SIZE, FRAME_SIZE, 3),\n",
        "        name='video_input'\n",
        "    )\n",
        "    x = layers.TimeDistributed(layers.Conv2D(4, (3,3)))(video_input)\n",
        "    x = layers.TimeDistributed(layers.MaxPooling2D(2))(x)\n",
        "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)\n",
        "    x = layers.GRU(8)(x)\n",
        "\n",
        "    # Audio pathway\n",
        "    audio_input = layers.Input(\n",
        "        shape=(MAX_TIME_STEPS, AUDIO_FEAT_DIM),\n",
        "        name='audio_input'\n",
        "    )\n",
        "    y = layers.Conv1D(8, 3, activation='relu')(audio_input)\n",
        "    y = layers.GlobalAveragePooling1D()(y)\n",
        "\n",
        "    # Fusion\n",
        "    z = layers.concatenate([x, y])\n",
        "    z = layers.Dense(8, activation='relu')(z)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(z)\n",
        "\n",
        "    model = models.Model(\n",
        "        inputs=[video_input, audio_input],\n",
        "        outputs=outputs,\n",
        "        name='multimodal_model'\n",
        "    )\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def load_and_prepare_data():\n",
        "    # Load all data with memory mapping\n",
        "    train_video = np.load(PATHS['train_video'], mmap_mode='r')\n",
        "    test_video = np.load(PATHS['test_video'], mmap_mode='r')\n",
        "    train_labels = np.load(PATHS['train_labels'])\n",
        "    test_labels = np.load(PATHS['test_labels'])\n",
        "\n",
        "    # Combine and split audio features\n",
        "    real_audio = np.load(PATHS['real_audio'], mmap_mode='r')\n",
        "    fake_audio = np.load(PATHS['fake_audio'], mmap_mode='r')\n",
        "\n",
        "    # Create aligned audio dataset\n",
        "    all_audio = np.concatenate([real_audio, fake_audio])  # (140, 1500, 1792)\n",
        "    train_audio = all_audio[:112]  # First 112 samples\n",
        "    test_audio = all_audio[112:]   # Last 28 samples\n",
        "\n",
        "    return (train_video, train_audio, train_labels), (test_video, test_audio, test_labels)\n",
        "\n",
        "def train_model():\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    # Load data with proper alignment\n",
        "    (train_vid, train_aud, train_lbl), (test_vid, test_aud, test_lbl) = load_and_prepare_data()\n",
        "\n",
        "    # Verify shapes\n",
        "    print(\"Data Shapes:\")\n",
        "    print(f\"Train Video: {train_vid.shape} | Audio: {train_aud.shape} | Labels: {train_lbl.shape}\")\n",
        "    print(f\"Test Video: {test_vid.shape} | Audio: {test_aud.shape} | Labels: {test_lbl.shape}\")\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = SafeDataGenerator(train_vid, train_aud, train_lbl, BATCH_SIZE)\n",
        "    test_gen = SafeDataGenerator(test_vid, test_aud, test_lbl, BATCH_SIZE)\n",
        "\n",
        "    # Create model\n",
        "    model = create_optimized_model()\n",
        "    model.summary()\n",
        "\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            validation_data=test_gen,\n",
        "            epochs=50,\n",
        "            callbacks=[\n",
        "                tf.keras.callbacks.EarlyStopping(patience=3),\n",
        "                tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True)\n",
        "            ],\n",
        "            verbose=1\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted. Saving partial model...\")\n",
        "        model.save('interrupted_model.keras')\n",
        "        return\n",
        "\n",
        "    # Final evaluation\n",
        "    model.load_weights('best_model.keras')\n",
        "    y_pred = (model.predict(test_gen) > 0.5).astype(int)\n",
        "    print(classification_report(test_lbl, y_pred))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gMTczebDFcpu",
        "outputId": "1f00a8d9-6087-44b0-f5e8-0b0734334eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shapes:\n",
            "Train Video: (112, 40, 64, 64, 3) | Audio: (112, 1500, 1792) | Labels: (112,)\n",
            "Test Video: (28, 40, 64, 64, 3) | Audio: (28, 1500, 1792) | Labels: (28,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"multimodal_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"multimodal_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " video_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m,              \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m3\u001b[0m)                                           \n",
              "\n",
              " time_distributed     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m62\u001b[0m,            \u001b[38;5;34m112\u001b[0m  video_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m4\u001b[0m)                                           \n",
              "\n",
              " time_distributed_1   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m31\u001b[0m,              \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m4\u001b[0m)                                           \n",
              "\n",
              " audio_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m,                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m1792\u001b[0m)                                            \n",
              "\n",
              " time_distributed_2   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m4\u001b[0m)               \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)                                                     \n",
              "\n",
              " conv1d (\u001b[38;5;33mConv1D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1498\u001b[0m, \u001b[38;5;34m8\u001b[0m)        \u001b[38;5;34m43,016\u001b[0m  audio_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " gru (\u001b[38;5;33mGRU\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 \u001b[38;5;34m336\u001b[0m  time_distributed \n",
              "\n",
              " global_average_poo  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   \u001b[38;5;34m0\u001b[0m  conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mGlobalAveragePool\u001b[0m                                                   \n",
              "\n",
              " concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                       global_average_p \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                 \u001b[38;5;34m136\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   \u001b[38;5;34m9\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " video_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                                           \n",
              "\n",
              " time_distributed     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>  video_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                           \n",
              "\n",
              " time_distributed_1   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                                           \n",
              "\n",
              " audio_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>,                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1792</span>)                                            \n",
              "\n",
              " time_distributed_2   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                                                     \n",
              "\n",
              " conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1498</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">43,016</span>  audio_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">336</span>  time_distributed \n",
              "\n",
              " global_average_poo  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool</span>                                                   \n",
              "\n",
              " concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       global_average_p \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">136</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,609\u001b[0m (170.35 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,609</span> (170.35 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,609\u001b[0m (170.35 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,609</span> (170.35 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 398ms/step - accuracy: 0.5216 - auc: 0.4599 - loss: 0.8490 - val_accuracy: 0.4286 - val_auc: 0.3980 - val_loss: 0.7189\n",
            "Epoch 2/50\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 323ms/step - accuracy: 0.4861 - auc: 0.5464 - loss: 0.6926 - val_accuracy: 0.5000 - val_auc: 0.4209 - val_loss: 0.7160\n",
            "Epoch 3/50\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 309ms/step - accuracy: 0.4871 - auc: 0.5432 - loss: 0.6843 - val_accuracy: 0.4286 - val_auc: 0.3929 - val_loss: 0.7330\n",
            "Epoch 4/50\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 346ms/step - accuracy: 0.5999 - auc: 0.6890 - loss: 0.6515 - val_accuracy: 0.3929 - val_auc: 0.3929 - val_loss: 0.7349\n",
            "Epoch 5/50\n",
            "\u001b[1m56/56\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 331ms/step - accuracy: 0.5820 - auc: 0.6848 - loss: 0.6706 - val_accuracy: 0.4286 - val_auc: 0.4082 - val_loss: 0.7471\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.43      0.46        14\n",
            "           1       0.50      0.57      0.53        14\n",
            "\n",
            "    accuracy                           0.50        28\n",
            "   macro avg       0.50      0.50      0.50        28\n",
            "weighted avg       0.50      0.50      0.50        28\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OPTION 2"
      ],
      "metadata": {
        "id": "JRnKZUQ0LD7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usV5SB6NpHxy",
        "outputId": "a9d6eef2-7471-4027-9f93-27c971d00762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from pydub import AudioSegment\n",
        "from transformers import (\n",
        "    Wav2Vec2Processor, Wav2Vec2Model,\n",
        "    WhisperFeatureExtractor, WhisperModel\n",
        ")\n",
        "\n",
        "# --- Configuration ---\n",
        "SAMPLE_RATE = 16000\n",
        "MAX_AUDIO_LENGTH = 30  # seconds\n",
        "MAX_TIME_STEPS = 1500\n",
        "WHISPER_MODEL_ID = \"openai/whisper-small\"\n",
        "\n",
        "LANG_TO_WAV2VEC2 = {\n",
        "    'en': 'facebook/wav2vec2-large-robust-ft-swbd-300h',\n",
        "    'fr': 'facebook/wav2vec2-large-xlsr-53-french',\n",
        "    'ru': 'anton-l/wav2vec2-large-xlsr-53-russian',\n",
        "    'es': 'facebook/wav2vec2-large-xlsr-53-spanish',\n",
        "    'zh': 'jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn',\n",
        "    'ja': 'jonatasgrosman/wav2vec2-large-xlsr-53-japanese',\n",
        "    'ar': 'jonatasgrosman/wav2vec2-large-xlsr-53-arabic'\n",
        "}\n",
        "\n",
        "# --- Initialization ---\n",
        "def init_processors_and_models(languages):\n",
        "    wav2vec2_processors = {}\n",
        "    wav2vec2_models = {}\n",
        "    for lang in languages:\n",
        "        model_id = LANG_TO_WAV2VEC2[lang]\n",
        "        wav2vec2_processors[lang] = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "        wav2vec2_models[lang] = Wav2Vec2Model.from_pretrained(model_id)\n",
        "    whisper_fe = WhisperFeatureExtractor.from_pretrained(WHISPER_MODEL_ID)\n",
        "    whisper_model = WhisperModel.from_pretrained(WHISPER_MODEL_ID).encoder\n",
        "    return wav2vec2_processors, wav2vec2_models, whisper_fe, whisper_model\n",
        "\n",
        "# --- Audio Extraction ---\n",
        "def extract_audio_from_video(video_path):\n",
        "    try:\n",
        "        temp_dir = os.path.join(os.getcwd(), \"temp\")\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "        temp_path = os.path.join(temp_dir, f\"temp_{os.path.basename(video_path)}.wav\")\n",
        "        audio = AudioSegment.from_file(video_path)\n",
        "        audio = audio.set_frame_rate(SAMPLE_RATE).set_channels(1)\n",
        "        target_length = MAX_AUDIO_LENGTH * 1000\n",
        "        if len(audio) < target_length:\n",
        "            silence = AudioSegment.silent(duration=target_length - len(audio))\n",
        "            audio += silence\n",
        "        else:\n",
        "            audio = audio[:target_length]\n",
        "        audio.export(temp_path, format=\"wav\")\n",
        "        return temp_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error in audio extraction: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# --- Feature Extraction ---\n",
        "def extract_audio_features(path, lang, wav2vec2_processors, wav2vec2_models,\n",
        "                           whisper_fe, whisper_model):\n",
        "    temp_file = None\n",
        "    try:\n",
        "        if path.lower().endswith('.mp4'):\n",
        "            temp_file = extract_audio_from_video(path)\n",
        "            if not temp_file or not os.path.exists(temp_file):\n",
        "                return None\n",
        "            audio_path = temp_file\n",
        "        else:\n",
        "            audio_path = path\n",
        "\n",
        "        audio, sr = librosa.load(audio_path, sr=SAMPLE_RATE)\n",
        "        target_samples = SAMPLE_RATE * MAX_AUDIO_LENGTH\n",
        "        if len(audio) < target_samples:\n",
        "            audio = np.pad(audio, (0, target_samples - len(audio)))\n",
        "        else:\n",
        "            audio = audio[:target_samples]\n",
        "\n",
        "        # Wav2Vec2\n",
        "        print(f\"[{lang}] Wav2Vec2 extracting...\")\n",
        "        wav_input = wav2vec2_processors[lang](audio, return_tensors=\"pt\", sampling_rate=SAMPLE_RATE)\n",
        "        with torch.no_grad():\n",
        "            wav_features = wav2vec2_models[lang](**wav_input).last_hidden_state.squeeze(0).numpy()\n",
        "\n",
        "        # Whisper\n",
        "        print(f\"[{lang}] Whisper extracting...\")\n",
        "        whisper_input = whisper_fe(audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\").input_features\n",
        "        with torch.no_grad():\n",
        "            whisper_features = whisper_model(whisper_input).last_hidden_state.squeeze(0).numpy()\n",
        "\n",
        "        # Spectral\n",
        "        print(f\"[{lang}] Spectral features extracting...\")\n",
        "        hop_length = int(len(audio) / MAX_TIME_STEPS)\n",
        "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, hop_length=hop_length)\n",
        "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr, hop_length=hop_length)\n",
        "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, hop_length=hop_length)\n",
        "        spectral = np.concatenate([mfcc, chroma, contrast], axis=0).T\n",
        "\n",
        "        # Align features\n",
        "        min_len = min(wav_features.shape[0], whisper_features.shape[0], spectral.shape[0])\n",
        "        wav_features = wav_features[:min_len]\n",
        "        whisper_features = whisper_features[:min_len]\n",
        "        spectral = spectral[:min_len]\n",
        "        combined = np.concatenate([wav_features, whisper_features, spectral], axis=1)\n",
        "\n",
        "        if combined.shape[0] < MAX_TIME_STEPS:\n",
        "            padding = np.zeros((MAX_TIME_STEPS - combined.shape[0], combined.shape[1]))\n",
        "            combined = np.vstack([combined, padding])\n",
        "        else:\n",
        "            combined = combined[:MAX_TIME_STEPS]\n",
        "\n",
        "        return combined\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Feature extraction error: {str(e)}\")\n",
        "        return None\n",
        "    finally:\n",
        "        if temp_file and os.path.exists(temp_file):\n",
        "            os.remove(temp_file)\n",
        "\n",
        "# --- Dataset Processor ---\n",
        "def process_directory(base_dir, wav2vec2_processors, wav2vec2_models, whisper_fe, whisper_model, is_fake=False):\n",
        "    features = []\n",
        "    labels = []\n",
        "    for folder in os.listdir(base_dir):\n",
        "        folder_path = os.path.join(base_dir, folder)\n",
        "        if not os.path.isdir(folder_path): continue\n",
        "\n",
        "        lang = folder.replace('to_', '') if is_fake else folder\n",
        "        print(f\"\\nProcessing folder: {folder_path} | Lang: {lang}\")\n",
        "        for file in os.listdir(folder_path):\n",
        "            if file.lower().endswith('.mp4'):\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "                feat = extract_audio_features(file_path, lang, wav2vec2_processors, wav2vec2_models, whisper_fe, whisper_model)\n",
        "                if feat is not None and feat.shape[0] == MAX_TIME_STEPS:\n",
        "                    features.append(feat)\n",
        "                    labels.append(1 if is_fake else 0)\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# --- Save to Drive ---\n",
        "def save_features_and_labels(features, labels, prefix, save_dir):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    f_path = os.path.join(save_dir, f\"{prefix}_features.npy\")\n",
        "    l_path = os.path.join(save_dir, f\"{prefix}_labels.npy\")\n",
        "    np.save(f_path, features)\n",
        "    np.save(l_path, labels)\n",
        "    print(f\"\\nSaved: {f_path} | {l_path}\")\n",
        "\n",
        "# --- Main Pipeline ---\n",
        "if __name__ == \"__main__\":\n",
        "    languages = ['en', 'fr', 'ru', 'es', 'zh', 'ja', 'ar']\n",
        "\n",
        "    # Verify Google Drive paths first\n",
        "    base_path = '/content/drive/MyDrive/PolyGlotFake2'\n",
        "    real_dir = os.path.join(base_path, 'real2')\n",
        "    fake_dir = os.path.join(base_path, 'fake2')\n",
        "    save_dir = os.path.join(base_path, 'processed_audio_v2')\n",
        "\n",
        "    # 1. Check if directories exist\n",
        "    for path in [real_dir, fake_dir]:\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Directory not found: {path}\\n\"\n",
        "                                  \"Please verify:\\n\"\n",
        "                                  \"1. Google Drive is properly mounted\\n\"\n",
        "                                  \"2. Folder names match exactly\\n\"\n",
        "                                  \"3. Data is organized correctly\")\n",
        "\n",
        "    # 2. Initialize models after verifying paths\n",
        "    print(\"Loading models...\")\n",
        "    wav2vec2_processors, wav2vec2_models, whisper_fe, whisper_model = init_processors_and_models(languages)\n",
        "\n",
        "    # 3. Process data with progress tracking\n",
        "    print(\"\\n--- Processing Real Data ---\")\n",
        "    real_features, real_labels = process_directory(\n",
        "        real_dir,\n",
        "        wav2vec2_processors,\n",
        "        wav2vec2_models,\n",
        "        whisper_fe,\n",
        "        whisper_model,\n",
        "        is_fake=False\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Processing Fake Data ---\")\n",
        "    fake_features, fake_labels = process_directory(\n",
        "        fake_dir,\n",
        "        wav2vec2_processors,\n",
        "        wav2vec2_models,\n",
        "        whisper_fe,\n",
        "        whisper_model,\n",
        "        is_fake=True\n",
        "    )\n",
        "\n",
        "    # 4. Save with verification\n",
        "    save_features_and_labels(real_features, real_labels, 'real_audio2', save_dir)\n",
        "    save_features_and_labels(fake_features, fake_labels, 'fake_audio2', save_dir)\n",
        "\n",
        "    print(f\"\\n[Done] Real: {real_features.shape}, Fake: {fake_features.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCy8dEfQor-L",
        "outputId": "a78b7696-29e5-4b97-a6e1-0a5c4ef34737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "\n",
            "--- Processing Real Data ---\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/ru | Lang: ru\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/en | Lang: en\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/ja | Lang: ja\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/fr | Lang: fr\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/es | Lang: es\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/zh | Lang: zh\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/real2/ar | Lang: ar\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "\n",
            "--- Processing Fake Data ---\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_zh | Lang: zh\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "[zh] Wav2Vec2 extracting...\n",
            "[zh] Whisper extracting...\n",
            "[zh] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_ru | Lang: ru\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "[ru] Wav2Vec2 extracting...\n",
            "[ru] Whisper extracting...\n",
            "[ru] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_ja | Lang: ja\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "[ja] Wav2Vec2 extracting...\n",
            "[ja] Whisper extracting...\n",
            "[ja] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_fr | Lang: fr\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "[fr] Wav2Vec2 extracting...\n",
            "[fr] Whisper extracting...\n",
            "[fr] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_es | Lang: es\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "[es] Wav2Vec2 extracting...\n",
            "[es] Whisper extracting...\n",
            "[es] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_ar | Lang: ar\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "[ar] Wav2Vec2 extracting...\n",
            "[ar] Whisper extracting...\n",
            "[ar] Spectral features extracting...\n",
            "\n",
            "Processing folder: /content/drive/MyDrive/PolyGlotFake2/fake2/to_en | Lang: en\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "[en] Wav2Vec2 extracting...\n",
            "[en] Whisper extracting...\n",
            "[en] Spectral features extracting...\n",
            "\n",
            "Saved: /content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/real_audio2_features.npy | /content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/real_audio2_labels.npy\n",
            "\n",
            "Saved: /content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/fake_audio2_features.npy | /content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/fake_audio2_labels.npy\n",
            "\n",
            "[Done] Real: (70, 1500, 1824), Fake: (70, 1500, 1824)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Configuration paths\n",
        "PATHS = {\n",
        "    'train_video': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_features_v2.npy',\n",
        "    'train_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_labels_v2.npy',\n",
        "    'test_video': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_features_v2.npy',\n",
        "    'test_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_labels_v2.npy',\n",
        "}\n",
        "\n",
        "# MediaPipe Face Mesh initialization\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "LIPS_INDICES = [\n",
        "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291,\n",
        "    375, 321, 405, 314, 17, 84, 181, 91, 146,\n",
        "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308,\n",
        "    324, 318, 402, 317, 14, 87, 178, 88, 95\n",
        "]\n",
        "\n",
        "class VideoProcessor:\n",
        "    def __init__(self):\n",
        "        self.face_mesh = mp_face_mesh.FaceMesh(\n",
        "            static_image_mode=False,\n",
        "            max_num_faces=1,\n",
        "            refine_landmarks=True,\n",
        "            min_detection_confidence=0.5,\n",
        "            min_tracking_confidence=0.5\n",
        "        )\n",
        "        self.flow_params = {\n",
        "            'pyr_scale': 0.5,\n",
        "            'levels': 3,\n",
        "            'winsize': 15,\n",
        "            'iterations': 3,\n",
        "            'poly_n': 5,\n",
        "            'poly_sigma': 1.2,\n",
        "            'flags': 0\n",
        "        }\n",
        "\n",
        "    def extract_lip_region(self, frame, face_landmarks):\n",
        "        \"\"\"Extract and align lip region with perspective correction\"\"\"\n",
        "        try:\n",
        "            h, w = frame.shape[:2]\n",
        "            lip_points = np.array([(lm.x * w, lm.y * h) for lm in face_landmarks.landmark])\n",
        "            lip_points = lip_points[LIPS_INDICES].astype(np.int32)\n",
        "\n",
        "            # Calculate bounding box with margin\n",
        "            x, y, w, h = cv2.boundingRect(lip_points)\n",
        "            margin = int(0.2 * max(w, h))\n",
        "            x = max(0, x - margin)\n",
        "            y = max(0, y - margin)\n",
        "\n",
        "            # Perspective warping\n",
        "            dst_points = np.array([[0, 0], [64, 0], [64, 64], [0, 64]], dtype=np.float32)\n",
        "            src_points = lip_points[[0, 10, 20, 30]].astype(np.float32)\n",
        "            M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
        "            return cv2.warpPerspective(frame, M, (64, 64))\n",
        "        except Exception:\n",
        "            return np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "    def calculate_optical_flow(self, prev_frame, curr_frame):\n",
        "        \"\"\"Calculate optical flow between frames with error handling\"\"\"\n",
        "        try:\n",
        "            prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "            curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "            return cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, **self.flow_params)\n",
        "        except Exception:\n",
        "            return np.zeros((64, 64, 2), dtype=np.float32)\n",
        "\n",
        "    def process_video(self, video_path, target_frames=40):\n",
        "        \"\"\"Process video with enhanced error handling and optical flow\"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            if not cap.isOpened():\n",
        "                return np.zeros((target_frames, 64, 64, 5), dtype=np.float32)\n",
        "\n",
        "            frames = []\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "            frame_interval = max(1, int(cap.get(cv2.CAP_PROP_FPS) // 5))\n",
        "            prev_lip = None  # Initialize as None\n",
        "\n",
        "            for frame_idx in range(0, total_frames, frame_interval):\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    continue\n",
        "\n",
        "                # Face detection with rotation attempts\n",
        "                results = None\n",
        "                for _ in range(3):  # Try rotated versions\n",
        "                    results = self.face_mesh.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "                    if results.multi_face_landmarks:\n",
        "                        break\n",
        "                    frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
        "\n",
        "                # Explicit check for valid results\n",
        "                if not results or not results.multi_face_landmarks:\n",
        "                    continue\n",
        "\n",
        "                # Safe landmark access\n",
        "                try:\n",
        "                    face_landmarks = results.multi_face_landmarks[0]\n",
        "                except (IndexError, TypeError):\n",
        "                    continue\n",
        "\n",
        "                lip_region = self.extract_lip_region(frame, face_landmarks)\n",
        "\n",
        "                # Corrected truth value check\n",
        "                if prev_lip is not None:\n",
        "                    flow = self.calculate_optical_flow(prev_lip, lip_region)\n",
        "                else:\n",
        "                    flow = np.zeros((64, 64, 2), dtype=np.float32)\n",
        "\n",
        "                frames.append((lip_region.astype(np.float32)/255.0, flow/64.0))\n",
        "                prev_lip = lip_region.copy()  # Use copy to prevent reference issues\n",
        "\n",
        "                if len(frames) >= target_frames:\n",
        "                    break\n",
        "\n",
        "            # Smart padding using last valid frame\n",
        "            if len(frames) < target_frames:\n",
        "                last_valid = frames[-1] if frames else (np.zeros((64,64,3)), np.zeros((64,64,2)))\n",
        "                padding = [last_valid] * (target_frames - len(frames))\n",
        "                frames.extend(padding)\n",
        "\n",
        "            # Combine features\n",
        "            rgb = np.array([f[0] for f in frames[:target_frames]])\n",
        "            flow = np.array([f[1] for f in frames[:target_frames]])\n",
        "            return np.concatenate([rgb, flow], axis=-1)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {video_path}: {str(e)}\")\n",
        "            return np.zeros((target_frames, 64, 64, 5), dtype=np.float32)\n",
        "        finally:\n",
        "            cap.release()\n",
        "\n",
        "def load_dataset(root_dir, debug=True):\n",
        "    \"\"\"Load dataset with comprehensive tracking and error handling\"\"\"\n",
        "    processor = VideoProcessor()\n",
        "    X, y = [], []\n",
        "    stats = {\n",
        "        'total': 0, 'processed': 0, 'skipped': 0,\n",
        "        'reasons': {}, 'skipped_files': []\n",
        "    }\n",
        "\n",
        "    # Count total videos\n",
        "    for label, folders in [('real', 'real2'), ('fake', 'fake2')]:\n",
        "        label_path = os.path.join(root_dir, folders)\n",
        "        if os.path.exists(label_path):\n",
        "            for lang in os.listdir(label_path):\n",
        "                lang_path = os.path.join(label_path, lang)\n",
        "                if os.path.isdir(lang_path):\n",
        "                    stats['total'] += len([\n",
        "                        f for f in os.listdir(lang_path)\n",
        "                        if f.lower().endswith('.mp4')\n",
        "                    ])\n",
        "\n",
        "    print(f\" Found {stats['total']} total videos\")\n",
        "\n",
        "    # Process videos\n",
        "    for label_idx, (label_name, folder) in enumerate([('real', 'real2'), ('fake', 'fake2')]):\n",
        "        label_path = os.path.join(root_dir, folder)\n",
        "        if not os.path.exists(label_path):\n",
        "            print(f\" Missing directory: {label_path}\")\n",
        "            stats['reasons']['missing_directory'] = stats['reasons'].get('missing_directory', 0) + 1\n",
        "            continue\n",
        "\n",
        "        for lang in os.listdir(label_path):\n",
        "            lang_path = os.path.join(label_path, lang)\n",
        "            if not os.path.isdir(lang_path):\n",
        "                continue\n",
        "\n",
        "            video_files = [f for f in os.listdir(lang_path) if f.lower().endswith('.mp4')]\n",
        "\n",
        "            for video_file in tqdm(video_files, desc=f\"Processing {label_name} ({lang})\"):\n",
        "                video_path = os.path.join(lang_path, video_file)\n",
        "                try:\n",
        "                    features = processor.process_video(video_path)\n",
        "                    if features is not None:\n",
        "                        X.append(features)\n",
        "                        y.append(label_idx)\n",
        "                        stats['processed'] += 1\n",
        "                    else:\n",
        "                        stats['skipped'] += 1\n",
        "                        stats['reasons']['processing_failed'] = stats['reasons'].get('processing_failed', 0) + 1\n",
        "                        stats['skipped_files'].append(video_path)\n",
        "                except Exception as e:\n",
        "                    stats['skipped'] += 1\n",
        "                    stats['reasons'][str(type(e).__name__)] = stats['reasons'].get(str(type(e).__name__), 0) + 1\n",
        "                    stats['skipped_files'].append(video_path)\n",
        "\n",
        "    # Print final report\n",
        "    print(\"\\n Processing Report:\")\n",
        "    print(f\" Successfully processed: {stats['processed']}/{stats['total']}\")\n",
        "    print(f\" Skipped: {stats['skipped']}/{stats['total']}\")\n",
        "\n",
        "    if stats['skipped'] > 0:\n",
        "        print(\"\\n Skip Reasons:\")\n",
        "        for reason, count in stats['reasons'].items():\n",
        "            print(f\"- {reason}: {count}\")\n",
        "\n",
        "    return np.array(X, dtype=np.float32), np.array(y)\n",
        "\n",
        "def save_features_and_labels(features, labels, prefix, save_dir=\"/content/drive/MyDrive/PolyGlotFake2/processed_data\"):\n",
        "    \"\"\"Save features and labels with versioning\"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    version = 0\n",
        "    while True:\n",
        "        features_path = os.path.join(save_dir, f'{prefix}_features_v{version}.npy')\n",
        "        labels_path = os.path.join(save_dir, f'{prefix}_labels_v{version}.npy')\n",
        "        if not (os.path.exists(features_path) or os.path.exists(labels_path)):\n",
        "            break\n",
        "        version += 1\n",
        "\n",
        "    np.save(features_path, features)\n",
        "    np.save(labels_path, labels)\n",
        "\n",
        "    print(f\" Saved version {version}:\")\n",
        "    print(f\"Features: {features_path} ({features.shape})\")\n",
        "    print(f\"Labels: {labels_path} ({labels.shape})\")\n",
        "    print(f\"Class balance: {np.sum(labels==0)} real, {np.sum(labels==1)} fake\")\n",
        "\n",
        "def load_features_and_labels(prefix, version=None, save_dir=PATHS['train_video'].rsplit('/', 1)[0]):\n",
        "    \"\"\"Load saved features and labels\"\"\"\n",
        "    if version == 'latest':\n",
        "        existing_files = [f for f in os.listdir(save_dir) if f.startswith(f'{prefix}_features_v')]\n",
        "        versions = [int(f.split('_v')[1].split('.npy')[0]) for f in existing_files]\n",
        "        version = max(versions) if versions else 0\n",
        "\n",
        "    features_path = os.path.join(save_dir, f'{prefix}_features_v{version}.npy')\n",
        "    labels_path = os.path.join(save_dir, f'{prefix}_labels_v{version}.npy')\n",
        "\n",
        "    features = np.load(features_path, allow_pickle=True)\n",
        "    labels = np.load(labels_path)\n",
        "\n",
        "    print(f\" Loaded version {version}:\")\n",
        "    print(f\"Features shape: {features.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATASET_PATH = \"/content/drive/MyDrive/PolyGlotFake2\"\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/PolyGlotFake2/processed_data\"\n",
        "\n",
        "    # Process dataset\n",
        "    try:\n",
        "        X, y = load_dataset(DATASET_PATH)\n",
        "\n",
        "        # Save processed data\n",
        "        if len(X) > 0:\n",
        "            # Save full dataset\n",
        "            save_features_and_labels(X, y, 'full', OUTPUT_DIR)\n",
        "\n",
        "            # Save train/test splits\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "            save_features_and_labels(X_train, y_train, 'train', OUTPUT_DIR)\n",
        "            save_features_and_labels(X_test, y_test, 'test', OUTPUT_DIR)\n",
        "        else:\n",
        "            print(\" No data to save - processing failed\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Critical error: {str(e)}\")\n",
        "        print(\" Verify dataset structure and file permissions\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czgCej_0-sWm",
        "outputId": "41594038-2876-45fa-afed-cd3041db3bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Found 140 total videos\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing real (ru): 100%|| 10/10 [01:20<00:00,  8.01s/it]\n",
            "Processing real (en): 100%|| 10/10 [01:08<00:00,  6.82s/it]\n",
            "Processing real (ja): 100%|| 10/10 [01:12<00:00,  7.27s/it]\n",
            "Processing real (fr): 100%|| 10/10 [01:17<00:00,  7.77s/it]\n",
            "Processing real (es): 100%|| 10/10 [01:38<00:00,  9.82s/it]\n",
            "Processing real (zh): 100%|| 10/10 [01:07<00:00,  6.76s/it]\n",
            "Processing real (ar): 100%|| 10/10 [01:24<00:00,  8.47s/it]\n",
            "Processing fake (to_zh): 100%|| 10/10 [02:47<00:00, 16.74s/it]\n",
            "Processing fake (to_ru): 100%|| 10/10 [02:51<00:00, 17.12s/it]\n",
            "Processing fake (to_ja): 100%|| 10/10 [02:32<00:00, 15.28s/it]\n",
            "Processing fake (to_fr): 100%|| 10/10 [02:44<00:00, 16.49s/it]\n",
            "Processing fake (to_es): 100%|| 10/10 [02:43<00:00, 16.34s/it]\n",
            "Processing fake (to_ar): 100%|| 10/10 [01:49<00:00, 10.92s/it]\n",
            "Processing fake (to_en): 100%|| 10/10 [02:43<00:00, 16.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Processing Report:\n",
            " Successfully processed: 140/140\n",
            " Skipped: 0/140\n",
            " Saved version 0:\n",
            "Features: /content/drive/MyDrive/PolyGlotFake2/processed_data/full_features_v0.npy ((140, 40, 64, 64, 5))\n",
            "Labels: /content/drive/MyDrive/PolyGlotFake2/processed_data/full_labels_v0.npy ((140,))\n",
            "Class balance: 70 real, 70 fake\n",
            " Saved version 3:\n",
            "Features: /content/drive/MyDrive/PolyGlotFake2/processed_data/train_features_v3.npy ((112, 40, 64, 64, 5))\n",
            "Labels: /content/drive/MyDrive/PolyGlotFake2/processed_data/train_labels_v3.npy ((112,))\n",
            "Class balance: 56 real, 56 fake\n",
            " Saved version 3:\n",
            "Features: /content/drive/MyDrive/PolyGlotFake2/processed_data/test_features_v3.npy ((28, 40, 64, 64, 5))\n",
            "Labels: /content/drive/MyDrive/PolyGlotFake2/processed_data/test_labels_v3.npy ((28,))\n",
            "Class balance: 14 real, 14 fake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import os\n",
        "\n",
        "# Configuration - UPDATED AUDIO DIMENSION\n",
        "BATCH_SIZE = 8\n",
        "SEQ_LENGTH = 40\n",
        "FRAME_SIZE = 64\n",
        "MAX_TIME_STEPS = 1500\n",
        "AUDIO_FEAT_DIM = 1824  # Corrected based on error message\n",
        "\n",
        "# Path configuration\n",
        "PATHS = {\n",
        "    'video': {\n",
        "        'train': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_features_v3.npy',\n",
        "        'test': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_features_v3.npy',\n",
        "        'train_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_labels_v3.npy',\n",
        "        'test_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_labels_v3.npy'\n",
        "    },\n",
        "    'audio': {\n",
        "        'real': '/content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/real_audio2_features.npy',\n",
        "        'fake': '/content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/fake_audio2_features.npy'\n",
        "    },\n",
        "    'output': '/content/drive/MyDrive/PolyGlotFake2/multimodal_models'\n",
        "}\n",
        "\n",
        "class MultimodalDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, video_data, audio_data, labels, batch_size, shuffle=True):\n",
        "        self.video_data = video_data\n",
        "        self.audio_data = audio_data\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(labels))\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "        video_batch = self.video_data[batch_indices].astype('float32') / 255.0\n",
        "        audio_batch = self.audio_data[batch_indices].astype('float32')\n",
        "        labels_batch = self.labels[batch_indices].astype('float32')\n",
        "\n",
        "        return {'video_input': video_batch, 'audio_input': audio_batch}, labels_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "def create_enhanced_model():\n",
        "    # Video processing branch (5 channels)\n",
        "    video_input = layers.Input(\n",
        "        shape=(SEQ_LENGTH, FRAME_SIZE, FRAME_SIZE, 5),\n",
        "        name='video_input'\n",
        "    )\n",
        "\n",
        "    # Temporal modeling\n",
        "    x = layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu'))(video_input)\n",
        "    x = layers.TimeDistributed(layers.MaxPooling2D(2))(x)\n",
        "    x = layers.TimeDistributed(layers.GlobalAveragePooling2D())(x)\n",
        "    x = layers.Bidirectional(layers.GRU(64, return_sequences=True))(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Audio processing branch (updated dimension)\n",
        "    audio_input = layers.Input(\n",
        "        shape=(MAX_TIME_STEPS, AUDIO_FEAT_DIM),  # Now using 1824 features\n",
        "        name='audio_input'\n",
        "    )\n",
        "\n",
        "    # Spectral analysis\n",
        "    y = layers.Conv1D(64, 3, activation='relu')(audio_input)\n",
        "    y = layers.MaxPooling1D(2)(y)\n",
        "    y = layers.Bidirectional(layers.GRU(64, return_sequences=True))(y)\n",
        "    y = layers.GlobalAveragePooling1D()(y)\n",
        "\n",
        "    # Multimodal fusion\n",
        "    fused = layers.concatenate([x, y])\n",
        "    fused = layers.Dense(128, activation='relu')(fused)\n",
        "    fused = layers.Dropout(0.5)(fused)\n",
        "    output = layers.Dense(1, activation='sigmoid')(fused)\n",
        "\n",
        "    model = models.Model(\n",
        "        inputs=[video_input, audio_input],\n",
        "        outputs=output,\n",
        "        name='enhanced_multimodal_model'\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=[\n",
        "            'accuracy',\n",
        "            tf.keras.metrics.AUC(name='auc'),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall')\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def load_and_align_data():\n",
        "    # Load video data\n",
        "    train_video = np.load(PATHS['video']['train'], mmap_mode='r')\n",
        "    test_video = np.load(PATHS['video']['test'], mmap_mode='r')\n",
        "    train_labels = np.load(PATHS['video']['train_labels'])\n",
        "    test_labels = np.load(PATHS['video']['test_labels'])\n",
        "\n",
        "    # Load and split audio data\n",
        "    real_audio = np.load(PATHS['audio']['real'], mmap_mode='r')\n",
        "    fake_audio = np.load(PATHS['audio']['fake'], mmap_mode='r')\n",
        "\n",
        "    # Verify audio dimensions\n",
        "    print(f\"Real audio shape: {real_audio.shape} (should be (N, {MAX_TIME_STEPS}, {AUDIO_FEAT_DIM}))\")\n",
        "    print(f\"Fake audio shape: {fake_audio.shape} (should be (N, {MAX_TIME_STEPS}, {AUDIO_FEAT_DIM}))\")\n",
        "\n",
        "    # Split audio to match video distribution\n",
        "    real_audio_train = real_audio[:56]  # 56 real train samples\n",
        "    real_audio_test = real_audio[56:70]  # 14 real test samples\n",
        "    fake_audio_train = fake_audio[:56]  # 56 fake train samples\n",
        "    fake_audio_test = fake_audio[56:70]  # 14 fake test samples\n",
        "\n",
        "    # Combine audio splits\n",
        "    audio_train = np.concatenate([real_audio_train, fake_audio_train])\n",
        "    audio_test = np.concatenate([real_audio_test, fake_audio_test])\n",
        "\n",
        "    # Verify final alignment\n",
        "    assert len(audio_train) == len(train_video), f\"Train mismatch: {len(audio_train)} vs {len(train_video)}\"\n",
        "    assert len(audio_test) == len(test_video), f\"Test mismatch: {len(audio_test)} vs {len(test_video)}\"\n",
        "\n",
        "    # Create validation split\n",
        "    X_train_vid, X_val_vid, X_train_aud, X_val_aud, y_train, y_val = train_test_split(\n",
        "        train_video, audio_train, train_labels,\n",
        "        test_size=0.2,\n",
        "        stratify=train_labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    return (X_train_vid, X_train_aud, y_train), (X_val_vid, X_val_aud, y_val), (test_video, audio_test, test_labels)\n",
        "\n",
        "def train_enhanced_model():\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    # Load data with shape verification\n",
        "    (train_vid, train_aud, train_lbl), (val_vid, val_aud, val_lbl), (test_vid, test_aud, test_lbl) = load_and_align_data()\n",
        "\n",
        "    # Print final data shapes\n",
        "    print(\"\\nFinal Data Shapes:\")\n",
        "    print(f\"Train Video: {train_vid.shape}, Audio: {train_aud.shape}, Labels: {train_lbl.shape}\")\n",
        "    print(f\"Val Video: {val_vid.shape}, Audio: {val_aud.shape}, Labels: {val_lbl.shape}\")\n",
        "    print(f\"Test Video: {test_vid.shape}, Audio: {test_aud.shape}, Labels: {test_lbl.shape}\")\n",
        "\n",
        "    # Create generators\n",
        "    train_gen = MultimodalDataGenerator(train_vid, train_aud, train_lbl, BATCH_SIZE)\n",
        "    val_gen = MultimodalDataGenerator(val_vid, val_aud, val_lbl, BATCH_SIZE, shuffle=False)\n",
        "    test_gen = MultimodalDataGenerator(test_vid, test_aud, test_lbl, BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Create model\n",
        "    model = create_enhanced_model()\n",
        "    model.summary()\n",
        "\n",
        "    # Callbacks\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(PATHS['output'], 'best_model.keras'),\n",
        "            save_best_only=True,\n",
        "            monitor='val_auc',\n",
        "            mode='max'\n",
        "        ),\n",
        "        tf.keras.callbacks.TensorBoard(\n",
        "            log_dir=os.path.join(PATHS['output'], 'logs'),\n",
        "            histogram_freq=1\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Training\n",
        "    try:\n",
        "        history = model.fit(\n",
        "            train_gen,\n",
        "            validation_data=val_gen,\n",
        "            epochs=50,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nTraining interrupted. Saving current weights...\")\n",
        "        model.save(os.path.join(PATHS['output'], 'interrupted_model.keras'))\n",
        "        return\n",
        "\n",
        "    # Final evaluation\n",
        "    model.load_weights(os.path.join(PATHS['output'], 'best_model.keras'))\n",
        "\n",
        "    # Test evaluation\n",
        "    print(\"\\nFinal Test Evaluation:\")\n",
        "    y_pred = (model.predict(test_gen) > 0.5).astype(int)\n",
        "    print(classification_report(test_lbl, y_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(PATHS['output'], exist_ok=True)\n",
        "    train_enhanced_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cuL6brIsPDr4",
        "outputId": "32f07741-67ad-48e2-ef99-f34eec7b7011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real audio shape: (70, 1500, 1824) (should be (N, 1500, 1824))\n",
            "Fake audio shape: (70, 1500, 1824) (should be (N, 1500, 1824))\n",
            "\n",
            "Final Data Shapes:\n",
            "Train Video: (89, 40, 64, 64, 5), Audio: (89, 1500, 1824), Labels: (89,)\n",
            "Val Video: (23, 40, 64, 64, 5), Audio: (23, 1500, 1824), Labels: (23,)\n",
            "Test Video: (28, 40, 64, 64, 5), Audio: (28, 1500, 1824), Labels: (28,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"enhanced_multimodal_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"enhanced_multimodal_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " video_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m,              \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m5\u001b[0m)                                           \n",
              "\n",
              " time_distributed     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m62\u001b[0m,          \u001b[38;5;34m1,472\u001b[0m  video_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                          \n",
              "\n",
              " audio_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m,                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m1824\u001b[0m)                                            \n",
              "\n",
              " time_distributed_1   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m31\u001b[0m,              \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)                                          \n",
              "\n",
              " conv1d (\u001b[38;5;33mConv1D\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1498\u001b[0m, \u001b[38;5;34m64\u001b[0m)      \u001b[38;5;34m350,272\u001b[0m  audio_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " time_distributed_2   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)                                                     \n",
              "\n",
              " max_pooling1d        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m749\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m0\u001b[0m  conv1d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      \n",
              " (\u001b[38;5;33mMaxPooling1D\u001b[0m)                                                        \n",
              "\n",
              " bidirectional        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m128\u001b[0m)        \u001b[38;5;34m37,632\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mBidirectional\u001b[0m)                                                       \n",
              "\n",
              " bidirectional_1      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m749\u001b[0m, \u001b[38;5;34m128\u001b[0m)       \u001b[38;5;34m49,920\u001b[0m  max_pooling1d[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mBidirectional\u001b[0m)                                                       \n",
              "\n",
              " global_average_poo  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  bidirectional[\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mGlobalAveragePool\u001b[0m                                                   \n",
              "\n",
              " global_average_poo  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  bidirectional_1[\u001b[38;5;34m\u001b[0m \n",
              " (\u001b[38;5;33mGlobalAveragePool\u001b[0m                                                   \n",
              "\n",
              " concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  global_average_p \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                       global_average_p \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m32,896\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " dropout (\u001b[38;5;33mDropout\u001b[0m)    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m  dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense_1 (\u001b[38;5;33mDense\u001b[0m)      (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 \u001b[38;5;34m129\u001b[0m  dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " video_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                                           \n",
              "\n",
              " time_distributed     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>,          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,472</span>  video_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                          \n",
              "\n",
              " audio_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>,                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1824</span>)                                            \n",
              "\n",
              " time_distributed_1   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                                          \n",
              "\n",
              " conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1498</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      <span style=\"color: #00af00; text-decoration-color: #00af00\">350,272</span>  audio_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " time_distributed_2   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                                                     \n",
              "\n",
              " max_pooling1d        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">749</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  conv1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)                                                        \n",
              "\n",
              " bidirectional        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        <span style=\"color: #00af00; text-decoration-color: #00af00\">37,632</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                                                       \n",
              "\n",
              " bidirectional_1      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">749</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       <span style=\"color: #00af00; text-decoration-color: #00af00\">49,920</span>  max_pooling1d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)                                                       \n",
              "\n",
              " global_average_poo  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  bidirectional[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool</span>                                                   \n",
              "\n",
              " global_average_poo  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\"></span> \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool</span>                                                   \n",
              "\n",
              " concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  global_average_p \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       global_average_p \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span>  dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m472,321\u001b[0m (1.80 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">472,321</span> (1.80 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m472,321\u001b[0m (1.80 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">472,321</span> (1.80 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m 1/12\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m5:27\u001b[0m 30s/step - accuracy: 0.0000e+00 - auc: 0.0000e+00 - loss: 0.7427 - precision: 0.0000e+00 - recall: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Configuration\n",
        "BATCH_SIZE = 4\n",
        "SEQ_LENGTH = 40\n",
        "FRAME_SIZE = 64\n",
        "MAX_TIME_STEPS = 1500\n",
        "AUDIO_FEAT_DIM = 1824\n",
        "\n",
        "PATHS = {\n",
        "    'video': {\n",
        "        'train': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_features_v3.npy',\n",
        "        'test': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_features_v3.npy',\n",
        "        'train_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/train_labels_v3.npy',\n",
        "        'test_labels': '/content/drive/MyDrive/PolyGlotFake2/processed_data/test_labels_v3.npy'\n",
        "    },\n",
        "    'audio': {\n",
        "        'real': '/content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/real_audio2_features.npy',\n",
        "        'fake': '/content/drive/MyDrive/PolyGlotFake2/processed_audio_v2/fake_audio2_features.npy'\n",
        "    },\n",
        "    'output': '/content/drive/MyDrive/PolyGlotFake2/multimodal_models'\n",
        "}\n",
        "\n",
        "class SafeDataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, video_path, audio_path, label_path, batch_size, shuffle=True):\n",
        "        self.video_data = np.load(video_path, mmap_mode='r', allow_pickle=True)\n",
        "        self.audio_data = np.load(audio_path, mmap_mode='r', allow_pickle=True)\n",
        "        self.labels = np.load(label_path, allow_pickle=True)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indices = np.arange(len(self.labels))\n",
        "        super().__init__()\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.labels) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
        "\n",
        "        video_batch = np.array(self.video_data[batch_indices], dtype=np.float32) / 255.0\n",
        "        audio_batch = np.array(self.audio_data[batch_indices], dtype=np.float32)\n",
        "        labels_batch = self.labels[batch_indices].astype(np.float32)\n",
        "\n",
        "        return {'video_input': video_batch, 'audio_input': audio_batch}, labels_batch\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "def create_streamlined_model():\n",
        "    # Video processing branch with named input\n",
        "    video_input = layers.Input(\n",
        "        shape=(SEQ_LENGTH, FRAME_SIZE, FRAME_SIZE, 5),\n",
        "        name='video_input'\n",
        "    )\n",
        "    x = layers.TimeDistributed(layers.Conv2D(16, (3,3), activation='relu'))(video_input)\n",
        "    x = layers.TimeDistributed(layers.GlobalAvgPool2D())(x)\n",
        "    x = layers.GRU(32)(x)\n",
        "\n",
        "    # Audio processing branch with named input\n",
        "    audio_input = layers.Input(\n",
        "        shape=(MAX_TIME_STEPS, AUDIO_FEAT_DIM),\n",
        "        name='audio_input'\n",
        "    )\n",
        "    y = layers.GRU(32)(audio_input)\n",
        "\n",
        "    # Feature fusion\n",
        "    fused = layers.concatenate([x, y])\n",
        "    output = layers.Dense(1, activation='sigmoid')(fused)\n",
        "\n",
        "    model = models.Model(\n",
        "        inputs=[video_input, audio_input],\n",
        "        outputs=output,\n",
        "        name='streamlined_multimodal_model'\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def align_audio_data():\n",
        "    # Create properly formatted audio datasets\n",
        "    real_audio = np.load(PATHS['audio']['real'], allow_pickle=True)\n",
        "    fake_audio = np.load(PATHS['audio']['fake'], allow_pickle=True)\n",
        "\n",
        "    # Train split: first 56 real + first 56 fake\n",
        "    train_audio = np.concatenate([real_audio[:56], fake_audio[:56]])\n",
        "    # Test split: next 14 real + next 14 fake\n",
        "    test_audio = np.concatenate([real_audio[56:70], fake_audio[56:70]])\n",
        "\n",
        "    # Save as new numpy files without pickling\n",
        "    np.save('train_audio_aligned.npy', train_audio, allow_pickle=False)\n",
        "    np.save('test_audio_aligned.npy', test_audio, allow_pickle=False)\n",
        "\n",
        "def train_model():\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    # Align audio data first\n",
        "    align_audio_data()\n",
        "\n",
        "    # Create data generators\n",
        "    train_gen = SafeDataGenerator(\n",
        "        PATHS['video']['train'],\n",
        "        'train_audio_aligned.npy',\n",
        "        PATHS['video']['train_labels'],\n",
        "        BATCH_SIZE\n",
        "    )\n",
        "\n",
        "    test_gen = SafeDataGenerator(\n",
        "        PATHS['video']['test'],\n",
        "        'test_audio_aligned.npy',\n",
        "        PATHS['video']['test_labels'],\n",
        "        BATCH_SIZE,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Create and train model\n",
        "    model = create_streamlined_model()\n",
        "    model.summary()\n",
        "\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        validation_data=test_gen,\n",
        "        epochs=30,\n",
        "        callbacks=[\n",
        "            tf.keras.callbacks.ModelCheckpoint(\n",
        "                os.path.join(PATHS['output'], 'final_model.keras'),\n",
        "                save_best_only=True,\n",
        "                monitor='val_auc',\n",
        "                mode='max'\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Final evaluation\n",
        "    model.load_weights(os.path.join(PATHS['output'], 'final_model.keras'))\n",
        "    y_pred = (model.predict(test_gen) > 0.5).astype(int)\n",
        "    print(classification_report(test_gen.labels, y_pred, target_names=['Real', 'Fake']))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.makedirs(PATHS['output'], exist_ok=True)\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o9dFsw0HD5mA",
        "outputId": "b82ce67f-6328-4b15-94a1-baac61c882bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"streamlined_multimodal_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"streamlined_multimodal_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
              "\n",
              " video_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m64\u001b[0m,              \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m5\u001b[0m)                                           \n",
              "\n",
              " time_distributed     (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m62\u001b[0m,            \u001b[38;5;34m736\u001b[0m  video_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)    \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m16\u001b[0m)                                          \n",
              "\n",
              " time_distributed_1   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m40\u001b[0m, \u001b[38;5;34m16\u001b[0m)              \u001b[38;5;34m0\u001b[0m  time_distributed \n",
              " (\u001b[38;5;33mTimeDistributed\u001b[0m)                                                     \n",
              "\n",
              " audio_input          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1500\u001b[0m,                \u001b[38;5;34m0\u001b[0m  -                 \n",
              " (\u001b[38;5;33mInputLayer\u001b[0m)         \u001b[38;5;34m1824\u001b[0m)                                            \n",
              "\n",
              " gru (\u001b[38;5;33mGRU\u001b[0m)            (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m4,800\u001b[0m  time_distributed \n",
              "\n",
              " gru_1 (\u001b[38;5;33mGRU\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            \u001b[38;5;34m178,368\u001b[0m  audio_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n",
              " concatenate          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m  gru[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        \n",
              " (\u001b[38;5;33mConcatenate\u001b[0m)                                       gru_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       \n",
              "\n",
              " dense (\u001b[38;5;33mDense\u001b[0m)        (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                  \u001b[38;5;34m65\u001b[0m  concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] \n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
              "\n",
              " video_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                                           \n",
              "\n",
              " time_distributed     (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>,            <span style=\"color: #00af00; text-decoration-color: #00af00\">736</span>  video_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)    <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                                          \n",
              "\n",
              " time_distributed_1   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  time_distributed \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                                                     \n",
              "\n",
              " audio_input          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1500</span>,                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1824</span>)                                            \n",
              "\n",
              " gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)            (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">4,800</span>  time_distributed \n",
              "\n",
              " gru_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">178,368</span>  audio_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              " concatenate          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  gru[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        \n",
              " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)                                       gru_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       \n",
              "\n",
              " dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span>  concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] \n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m183,969\u001b[0m (718.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183,969</span> (718.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m183,969\u001b[0m (718.63 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183,969</span> (718.63 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 3s/step - accuracy: 0.4339 - auc: 0.4383 - loss: 0.7048 - val_accuracy: 0.4286 - val_auc: 0.4796 - val_loss: 0.6974\n",
            "Epoch 2/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 3s/step - accuracy: 0.5665 - auc: 0.5374 - loss: 0.6858 - val_accuracy: 0.4643 - val_auc: 0.4923 - val_loss: 0.6963\n",
            "Epoch 3/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.4879 - auc: 0.5848 - loss: 0.6822 - val_accuracy: 0.4286 - val_auc: 0.5026 - val_loss: 0.6964\n",
            "Epoch 4/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 0.5904 - auc: 0.7106 - loss: 0.6643 - val_accuracy: 0.4643 - val_auc: 0.4872 - val_loss: 0.6955\n",
            "Epoch 5/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.5964 - auc: 0.6633 - loss: 0.6662 - val_accuracy: 0.4643 - val_auc: 0.4847 - val_loss: 0.6958\n",
            "Epoch 6/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.6521 - auc: 0.7881 - loss: 0.6390 - val_accuracy: 0.5000 - val_auc: 0.4949 - val_loss: 0.6956\n",
            "Epoch 7/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.7328 - auc: 0.8646 - loss: 0.6206 - val_accuracy: 0.5357 - val_auc: 0.5281 - val_loss: 0.6955\n",
            "Epoch 8/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 0.7451 - auc: 0.8741 - loss: 0.6218 - val_accuracy: 0.5714 - val_auc: 0.4949 - val_loss: 0.6982\n",
            "Epoch 9/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 0.8976 - auc: 0.9565 - loss: 0.5822 - val_accuracy: 0.5000 - val_auc: 0.5128 - val_loss: 0.6981\n",
            "Epoch 10/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.8946 - auc: 0.9649 - loss: 0.5800 - val_accuracy: 0.5000 - val_auc: 0.4949 - val_loss: 0.6989\n",
            "Epoch 11/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 0.9151 - auc: 0.9804 - loss: 0.5591 - val_accuracy: 0.5357 - val_auc: 0.5051 - val_loss: 0.6971\n",
            "Epoch 12/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 3s/step - accuracy: 0.9457 - auc: 0.9583 - loss: 0.5374 - val_accuracy: 0.5357 - val_auc: 0.5128 - val_loss: 0.6980\n",
            "Epoch 13/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.9432 - auc: 0.9629 - loss: 0.5147 - val_accuracy: 0.5714 - val_auc: 0.4745 - val_loss: 0.6978\n",
            "Epoch 14/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 0.9817 - auc: 0.9989 - loss: 0.4786 - val_accuracy: 0.4643 - val_auc: 0.4745 - val_loss: 0.6989\n",
            "Epoch 15/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.9909 - auc: 0.9999 - loss: 0.4752 - val_accuracy: 0.5357 - val_auc: 0.4668 - val_loss: 0.7003\n",
            "Epoch 16/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 0.9832 - auc: 1.0000 - loss: 0.4478 - val_accuracy: 0.4643 - val_auc: 0.5000 - val_loss: 0.6981\n",
            "Epoch 17/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.4390 - val_accuracy: 0.4643 - val_auc: 0.5051 - val_loss: 0.6988\n",
            "Epoch 18/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3743 - val_accuracy: 0.4643 - val_auc: 0.4821 - val_loss: 0.7030\n",
            "Epoch 19/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3650 - val_accuracy: 0.4643 - val_auc: 0.4872 - val_loss: 0.7031\n",
            "Epoch 20/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3653 - val_accuracy: 0.4643 - val_auc: 0.4974 - val_loss: 0.7027\n",
            "Epoch 21/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3362 - val_accuracy: 0.5000 - val_auc: 0.5051 - val_loss: 0.7035\n",
            "Epoch 22/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.3173 - val_accuracy: 0.5000 - val_auc: 0.5255 - val_loss: 0.7010\n",
            "Epoch 23/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2871 - val_accuracy: 0.5000 - val_auc: 0.5153 - val_loss: 0.7052\n",
            "Epoch 24/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2573 - val_accuracy: 0.4643 - val_auc: 0.5434 - val_loss: 0.7014\n",
            "Epoch 25/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2301 - val_accuracy: 0.5000 - val_auc: 0.5306 - val_loss: 0.7064\n",
            "Epoch 26/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2085 - val_accuracy: 0.5000 - val_auc: 0.5332 - val_loss: 0.7072\n",
            "Epoch 27/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 2s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.2082 - val_accuracy: 0.5000 - val_auc: 0.5332 - val_loss: 0.7075\n",
            "Epoch 28/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1983 - val_accuracy: 0.5000 - val_auc: 0.5255 - val_loss: 0.7137\n",
            "Epoch 29/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 1.0000 - loss: 0.1841 - val_accuracy: 0.5000 - val_auc: 0.5408 - val_loss: 0.7106\n",
            "Epoch 30/30\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 3s/step - accuracy: 1.0000 - auc: 0.9655 - loss: 0.1632 - val_accuracy: 0.5357 - val_auc: 0.5485 - val_loss: 0.7128\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 349ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real       0.54      0.50      0.52        14\n",
            "        Fake       0.53      0.57      0.55        14\n",
            "\n",
            "    accuracy                           0.54        28\n",
            "   macro avg       0.54      0.54      0.54        28\n",
            "weighted avg       0.54      0.54      0.54        28\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}